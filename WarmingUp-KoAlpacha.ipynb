{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed95516-baee-4a26-9926-2ac9a058a660",
   "metadata": {},
   "source": [
    "Code Ref\n",
    "- https://colab.research.google.com/drive/1OQKVceFY_rx4Y74fXlvxyPGAkeyY3Rp9?usp=sharing#scrollTo=E5LC_9h5Vn-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1c1093-f428-4b77-927b-5848f5842d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes datasets loralib sentencepiece git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git gradio==3.20.0 tenacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba553c1-ca28-4814-8d49-fff56e350758",
   "metadata": {},
   "source": [
    "### Change default HF Model Download location to new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712f2c52-f223-4457-9928-05bcf4390387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_cache_dir = \"/home/ec2-user/SageMaker\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467fbb32-4521-48d1-a21b-2db3f0969fb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010857105255126953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 0,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a1476e52a14cb283e9e6ee1bbe3253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so.11.0'), PosixPath('/home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117_nocublaslt.so...\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "\n",
    "base = 'decapoda-research/llama-13b-hf'\n",
    "finetuned = 'beomi/KoAlpaca-13B-LoRA'\n",
    "# finetuned = 'chansung/koalpaca-lora-13b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    base,\n",
    "    cache_dir = model_cache_dir\n",
    ")\n",
    "tokenizer.pad_token_id = 0\n",
    "tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59e169d3-d935-438d-a59b-9b1d703f3e82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008056402206420898,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 41,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b7119262714bdab8e81bb1902ff9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir = model_cache_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3cf2c0e-c633-4eb7-a17b-42322e280397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(model, finetuned, device_map={'': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36d0909-efaa-4585-b933-390ebdc77e32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8de5e2-df61-4503-ab75-e4930f4a441a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a history of instructions that describe tasks, paired with an input that provides further context. Write a response that appropriately completes the request by remembering the conversation history.\n",
      "아래는 작업을 설명하는 지시 사항의 히스토리와 추가 컨텍스트를 제공하는 입력이 짝을 이루고 있습니다. 대화 기록을 기억하여 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Input: \n",
      "\n",
      "### Instruction: 소수(Prime number)를 순서대로 출력하는 파이썬 코드를 작성해주세요.\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "input = \"\"\n",
    "prompt = \"소수(Prime number)를 순서대로 출력하는 파이썬 코드를 작성해주세요.\"\n",
    "\n",
    "history = f\"\"\"Below is a history of instructions that describe tasks, paired with an input that provides further context. Write a response that appropriately completes the request by remembering the conversation history.\n",
    "아래는 작업을 설명하는 지시 사항의 히스토리와 추가 컨텍스트를 제공하는 입력이 짝을 이루고 있습니다. 대화 기록을 기억하여 요청을 적절히 완료하는 응답을 작성하세요.\n",
    "\n",
    "### Input: {input}\n",
    "\n",
    "### Instruction: {prompt}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d91bf098-da09-42c7-9e38-f78c557ed33b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/generation/utils.py:1255: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = 128\n",
    "temperature = 0.5\n",
    "\n",
    "input_ids = tokenizer.encode(history, return_tensors=\"pt\").to('cuda:0')\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    num_return_sequences=1, \n",
    "    temperature=temperature,\n",
    "    no_repeat_ngram_size=6,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb356793-df4e-45ec-9880-8c16e534358d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "```\n",
      "for(int i = 2; i < 100; i++)\n",
      "{\n",
      "    if(i % 2 == 0)\n",
      "    {\n",
      "        continue;\n",
      "    }\n",
      "    else\n",
      "    {\n",
      "        printf(\"%d\", i);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### Input-Output: \n",
      "\n",
      "``` \n",
      "입렴: \n",
      "```\n",
      "\n",
      "```\n",
      "출렴: 2 3 5 7 11 13 17 19 23 29\n"
     ]
    }
   ],
   "source": [
    "print(gen_text.replace(history, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6ffe785-fb8f-49ae-8796-1065903049a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a history of instructions that describe tasks, paired with an input that provides further context. Write a response that appropriately completes the request by remembering the conversation history.\n",
      "아래는 작업을 설명하는 지시 사항의 히스토리와 추가 컨텍스트를 제공하는 입력이 짝을 이루고 있습니다. 대화 기록을 기억하여 요청을 적절히 완료하는 응답을 작성하세요.\n",
      "\n",
      "### Input: \n",
      "\n",
      "### Instruction: 소수(Prime number)를 순서대로 출력하는 파이썬 코드를 작성해주세요.\n",
      "\n",
      "### Response: \n",
      "\n",
      "```\n",
      "for(int i = 2; i < 100; i++)\n",
      "{\n",
      "    if(i % 2 == 0)\n",
      "    {\n",
      "        continue;\n",
      "    }\n",
      "    else\n",
      "    {\n",
      "        printf(\"%d\", i);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "### Input-Output: \n",
      "\n",
      "``` \n",
      "입렴: \n",
      "```\n",
      "\n",
      "```\n",
      "출렴: 2 3 5 7 11 13 17 19 23 29\n",
      "\n",
      "### Instruction: 위 코드를 자바스크립트로 변환해주세요.\n",
      "\n",
      "### Response:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"위 코드를 자바스크립트로 변환해주세요.\"\n",
    "\n",
    "history = gen_text + f\"\"\"\n",
    "\n",
    "### Instruction: {prompt}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e5f7e6-b55f-4e86-a9e6-fa254e8e23ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```\n",
      "```\n",
      "for(let i = 2; \n",
      "     i < 10;\n",
      "     i++)\n",
      "{\n",
      "     if(i % 3 == 0)\n",
      "     {\n",
      "         continue;\n",
      "     }\n",
      "     else\n",
      "     {\n",
      "         console.log(i);\n",
      "     }\n",
      " }\n",
      "```\n",
      "```\n",
      "추가 배열: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29\n",
      "```\n",
      "```\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(history, return_tensors=\"pt\").to('cuda:0')\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids=input_ids, \n",
    "    max_new_tokens=max_new_tokens, \n",
    "    num_return_sequences=1, \n",
    "    temperature=temperature,\n",
    "    no_repeat_ngram_size=6,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(gen_text.replace(history, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a21a81-7547-4233-b540-cb10fb538d72",
   "metadata": {},
   "source": [
    "## GPT4ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06dcae2-7f9b-4078-b764-a8fa0eb26cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes datasets loralib sentencepiece git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git gradio==3.20.0 tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c88a047f-671a-42f0-89d7-df14ef0dd31b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008162975311279297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)/adapter_config.json",
       "rate": null,
       "total": 352,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6fd37199ab46e181e10f21f1be159c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/adapter_config.json:   0%|          | 0.00/352 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00822591781616211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)lve/main/config.json",
       "rate": null,
       "total": 548,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e902397ded45cb8d4aca825f692a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0077419281005859375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)model.bin.index.json",
       "rate": null,
       "total": 26788,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412087d1dd7a40bf88438665ec70f169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007338523864746094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a57200551b4334b42af6661be33042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007429838180541992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00001-of-00003.bin",
       "rate": null,
       "total": 9877989586,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a7124d6066443ebbda7c0aaa93401b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009246826171875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00002-of-00003.bin",
       "rate": null,
       "total": 9894801014,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d9e1cf5b4c4dbbaad516c36cd1b2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007597684860229492,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)l-00003-of-00003.bin",
       "rate": null,
       "total": 7180990649,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90f597fe7fb41a294a92ce38c91291b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/7.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007545948028564453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241e31403bc34be19b4c614ec760e3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007524251937866211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)neration_config.json",
       "rate": null,
       "total": 137,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46bc51d73b94524a6f6433e8fec386a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0077419281005859375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)okenizer_config.json",
       "rate": null,
       "total": 217,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f81b6535524c63b437ed78cb82b5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007313966751098633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.model",
       "rate": null,
       "total": 499723,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca603aff5ed422191cdbefa871f435e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007430315017700195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "rate": null,
       "total": 3,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90cd0e42dab64936bfff6463fbd9f340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007752180099487305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading adapter_model.bin",
       "rate": null,
       "total": 8408957,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c211e0a8c8436f80d5c20a1d7dc70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_model.bin:   0%|          | 0.00/8.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import textwrap\n",
    "\n",
    "peft_model_id = \"nomic-ai/gpt4all-lora\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path, \n",
    "    return_dict=True, \n",
    "    load_in_8bit=True, \n",
    "    device_map='auto',\n",
    "    cache_dir = model_cache_dir    \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path,\n",
    "                                          cache_dir = model_cache_dir\n",
    "                                         )\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id,\n",
    "                                  cache_dir = model_cache_dir\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "891eec4c-fa44-4660-a7f2-017d9d9241be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gpt4all_generate(text):\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    print(\"Generating...\")\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        # return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256,\n",
    "    )\n",
    "\n",
    "    wrapped_text = textwrap.fill(tokenizer.decode(generation_output[0]), width=100)\n",
    "    \n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce045b33-7720-460a-a564-5ba521e1343c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "<s> Below is a history of instructions that describe tasks, paired with an input that provides\n",
      "further context. Write a response that appropriately completes the request by remembering the\n",
      "conversation history.  ### Instruction:Tell me about yourself.  ### Response: I am currently\n",
      "studying computer science and have always had a passion for technology. My hobbies include playing\n",
      "video games and watching movies.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\bTell me about yourself.\"\n",
    "\n",
    "history = f\"\"\"Below is a history of instructions that describe tasks, paired with an input that provides further context. Write a response that appropriately completes the request by remembering the conversation history.\n",
    "\n",
    "### Instruction: {prompt}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "gen_text = gpt4all_generate(history)\n",
    "\n",
    "print(gen_text.replace(history, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dec25925-7b64-4421-921e-87eeb5237888",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "<s><s> Below is a history of instructions that describe tasks, paired with an input that provides\n",
      "further context. Write a response that appropriately completes the request by remembering the\n",
      "conversation history.  ### Instruction:Tell me about yourself.  ### Response: I am currently\n",
      "studying computer science and have always had a passion for technology. My hobbies include playing\n",
      "video games and watching movies.</s>  ### Instruction: 한글로 번역해주세요.  ### Response: ��가 필 하나 �� ������\n",
      "����������������������������������������������������������������������������������������������������\n",
      "����������������������������������������������������������������������������������������������������\n",
      "�����������������������������������\n"
     ]
    }
   ],
   "source": [
    "prompt = \"한글로 번역해주세요.\"\n",
    "\n",
    "history = gen_text + f\"\"\"\n",
    "\n",
    "### Instruction: {prompt}\n",
    "\n",
    "### Response:\"\"\"\n",
    "\n",
    "gen_text = gpt4all_generate(history)\n",
    "\n",
    "print(gen_text.replace(history, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5a790-ff2d-4377-82f4-86e75957e434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
