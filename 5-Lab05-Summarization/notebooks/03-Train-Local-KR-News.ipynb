{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# 로컬에서 훈련 하기\n",
    "- https://www.kaggle.com/code/mitanshuchakrawarty/fine-tune-llm-for-text-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06b5f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/SageMaker/.cache/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "!huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdcebd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/home/ec2-user/SageMaker/.cache\" \n",
    "os.environ['HF_DATASETS_CACHE'] = \"/home/ec2-user/SageMaker/.cache\" \n",
    "os.environ['HF_HOME'] = \"/home/ec2-user/SageMaker/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24bce431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_folder:  ../data/naver-news-summarization-ko\n",
      "train_data_json:  ../data/naver-news-summarization-ko/train/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test/test_dataset.json\n",
      "full_train_data_json:  ../data/naver-news-summarization-ko/full_train/train_dataset.json\n",
      "full_validation_data_json:  ../data/naver-news-summarization-ko/full_validation/validation_dataset.json\n",
      "full_test_data_json:  ../data/naver-news-summarization-ko/full_test/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "%store -r data_folder\n",
    "%store -r train_data_json \n",
    "%store -r validation_data_json \n",
    "%store -r test_data_json \n",
    "%store -r full_train_data_json \n",
    "%store -r full_validation_data_json \n",
    "%store -r full_test_data_json\n",
    "\n",
    "\n",
    "print(\"data_folder: \", data_folder)\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)\n",
    "print(\"full_train_data_json: \", full_train_data_json)\n",
    "print(\"full_validation_data_json: \", full_validation_data_json)\n",
    "print(\"full_test_data_json: \", full_test_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78f53c-d21d-4fca-b69d-6a85d966353c",
   "metadata": {},
   "source": [
    "## 2. 베이스 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d514e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "output_dir = \"/home/ec2-user/SageMaker/models/llama-3-8b-naver-news\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2403f6b8",
   "metadata": {},
   "source": [
    "### Config YAML 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d30120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting local_llama_3_8b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile local_llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id:  \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "# small samples for Debug\n",
    "# train_dataset_path: \"../data/naver-news-summarization-ko/train\"                      # path to dataset\n",
    "# validation_dataset_path: \"../data/naver-news-summarization-ko/validation\"                      # path to dataset\n",
    "# test_dataset_path: \"../data/naver-news-summarization-ko/test\"                      # path to dataset\n",
    "# large samples for evaluation\n",
    "train_dataset_path: \"../data/naver-news-summarization-ko/full_train\"                      # path to dataset\n",
    "validation_dataset_path: \"../data/naver-news-summarization-ko/full_validation\"                      # path to dataset\n",
    "test_dataset_path: \"../data/naver-news-summarization-ko/full_test\"                      # path to dataset\n",
    "max_seq_len:  2048              # max sequence length for model and packing of the dataset\n",
    "# training parameters\n",
    "output_dir: \"/home/ec2-user/SageMaker/models/llama-3-8b-naver-news\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 1                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851f75d",
   "metadata": {},
   "source": [
    "## 3. 훈련 Script 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-29 15:07:52,825] torch.distributed.run: [WARNING] \n",
      "[2024-06-29 15:07:52,825] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-06-29 15:07:52,825] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-06-29 15:07:52,825] torch.distributed.run: [WARNING] *****************************************\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "## script_args: \n",
      " ScriptArguments(train_dataset_path='../data/naver-news-summarization-ko/full_train', validation_dataset_path='../data/naver-news-summarization-ko/full_validation', model_id='meta-llama/Meta-Llama-3-8B', max_seq_length=512)\n",
      "## training_args: \n",
      " TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[<FSDPOption.FULL_SHARD: 'full_shard'>, <FSDPOption.AUTO_WRAP: 'auto_wrap'>, <FSDPOption.OFFLOAD: 'offload'>],\n",
      "fsdp_config={'backward_prefetch': 'backward_pre', 'forward_prefetch': 'false', 'use_orig_params': 'false', 'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/home/ec2-user/SageMaker/models/llama-3-8b-naver-news/runs/Jun29_15-07-55_ip-172-16-48-49.ec2.internal,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=constant,\n",
      "max_grad_norm=0.3,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=/home/ec2-user/SageMaker/models/llama-3-8b-naver-news,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/home/ec2-user/SageMaker/models/llama-3-8b-naver-news,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=True,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.03,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "Generating train split: 100 examples [00:00, 26833.24 examples/s]\n",
      "Generating train split: 100 examples [00:00, 36889.22 examples/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Map: 100%|███████████████████████████| 100/100 [00:00<00:00, 2882.35 examples/s]\n",
      "Map: 100%|███████████████████████████| 100/100 [00:00<00:00, 7857.15 examples/s]\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|end_of_text|>\n",
      "\n",
      "Assistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|end_of_text|>\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "기준금리 첫 0.5%P 인상 관측 두 달 연속 ‘빅스텝’ 단행 가능성 유동성 줄여도 인플레 영향 제한 고강도 긴축에 서민 고통 커져 전문가 “취약층 부채감면 고려” 이복현 ‘이자 장사’ 경고에… 예적금 금리 속속 인상 이복현 금융감독원장의 ‘이자 장사’ 경고가 나온 이후 시중은행들이 대출 금리 인하와 예적금 금리 인상에 나선 가운데 4일 경기 수원에서 한 여성이 폐지를 잔뜩 실은 손수레를 끌고 은행 금리 안내문이 외벽에 붙은 공사장 앞을 지나고 있다.뉴스1 지난달 소비자물가 상승률이 6%대에 달할 것이란 관측이 이어지면서 한국은행이 기준금리를 한 번에 0.5% 포인트 올리는 ‘빅스텝’을 사상 처음으로 단행할 것이란 전망이 지배적이다. 이달과 다음달 두 달 연속 빅스텝을 밟을 가능성도 제기된다. 한은이 긴축의 고삐를 바짝 조이는 것은 물가 안정이란 책무를 이행하기 위한 것이지만 효과가 크지 않을 것이란 전망도 많다. 지금의 인플레이션은 공급과 대외적 요인이 주요 원인으로 작용하고 있기 때문이다. 금리 인상이 물가는 잡지 못하면서 가계와 기업 등 경제주체에 대한 고통만 가중시키는 것 아니냐는 우려도 있다. MobileAdNew center 양준석 가톨릭대 경제학과 교수는 4일 “빅스텝을 단행해도 물가를 안정시키는 효과는 거의 없을 것으로 본다”며 “금리를 올리고 유동성을 줄이면 물가를 자극하는 수요를 일부 억제할 순 있지만 가장 큰 인플레이션 야기 요인인 대외적 요인엔 영향을 끼치지 못한다”고 말했다. 양 교수는 그럼에도 빅스텝은 필요하다고 덧붙였는데 한미 금리 역전에 따른 외국인 자본 이탈이 우려되기 때문이다. 지나치게 강도 높은 긴축은 경제주체를 고통에 빠뜨리고 경기를 침체시키는 등 부작용을 초래할 수 있다는 지적도 나온다. ‘인플레이션 파이터’ 폴 볼커 전 미 연방준비제도이사회 연준 의장은 스태그플레이션 경기침체 속 물가 상승 이 극성을 부리던 1979년 11.5%였던 기준금리를 1981년 20%까지 끌어올렸다. 이런 조치는 물가를 잡는 데 성공했지만 기업이 줄도산하고 실업률이 11%까지 치솟는 등 미국 경기를 냉각기에 빠뜨렸다. 볼커의 조치가 효과적이었는지는 지금도 논쟁 대상이다. 한국개발연구원 KDI 도 최근 보고서에서 고물가로 인해 기준금리 인상이 필요하지만 미국 금리 인상 속도를 따라잡기 위해 과도하게 높일 필요는 없다고 제언했다. 정규철 경제전망실장은 “미국을 따라 금리를 인상하면 경기 둔화가 그대로 파급되지만 독립적인 통화정책을 수행할 경우엔 일시적인 물가상승 외엔 큰 영향을 받지 않는다”며 “한미 기준금리 격차는 용인할 필요가 있다”고 했다. 박창균 자본시장연구원 연구조정실장은 “‘공짜 점심은 없다’는 경제학 격언처럼 물가를 안정시키려면 고통스럽더라도 강도 높은 긴축을 견뎌야 한다”며 “다만 가파른 금리 인상을 버티기 힘든 자영업자와 저소득층 등 취약계층을 보듬는 정책적 노력을 정부가 펼쳐야 한다”고 제언했다. 재정을 풀어 이들을 지원하거나 부채 감면 등의 조치를 취해야 한다는 것이다. 앞서 이창용 한은 총재는 “빅스텝은 물가 하나만 보고 결정하는 게 아니다”라며 “물가가 올랐을 때 우리 경기나 환율에 미치는 영향도 봐야 한다”고 말했다.<|end_of_text|>\n",
      "\n",
      "Assistant: 한국은행이 기준금리를 한 번에 0.5% 포인트 올리는 ‘빅스텝’을 사상 처음으로 단행할 것이란 전망에 대해 MobileAdNew center 양준석 가톨릭대 경제학과 교수는 데이터를 올리고 유동성을 줄이면 물가를 자극하는 수요를 일부 억제할 순 있지만 가장 큰 인플레이션 야기 요인인 대외적 요인엔 영향을 끼치지 못한다”고 말했으며 한국개발연구원 KDI 도 최근 보고서에서 고물가로 인해 기준금리 인상이 필요하지만 미국 금리 인상 속도를 따라잡기 위해 과도하게 높일 필요는 없다고 제언했다.<|end_of_text|>\n",
      "Map: 100%|███████████████████████████| 100/100 [00:00<00:00, 2928.29 examples/s]\n",
      "Map: 100%|███████████████████████████| 100/100 [00:00<00:00, 2925.04 examples/s]\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|end_of_text|>\n",
      "\n",
      "Assistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|end_of_text|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|end_of_text|>\n",
      "\n",
      "Assistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|end_of_text|>\n",
      "\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "국내 최대규모 나노 전시회이자 세계 3대 나노행사 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀 에서 개최된다. 올해 20회를 맞는 나노코리아는 산업통상자원부 과학기술정보통신부가 공동 주최한다. 나노융합산업연구조합과 나노기술연구협의회가 주관한다. 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다. 강민석 LG이노텍 부사장이 자율주행산업 동향에 따른 나노기술과 인공지능 AI 의 활용 을 주제로 기조강연을 한다. 알베르페르 프랑스 파리 슈드대 교수도 기조 강연을 맡는다. 전시규모는 코로나19 이전 수준으로 회복됐다. 삼성전자 LG 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여한다. 나노 20주년 특별 기념관도 마련된다. 20주년 특별 기념관에는 차세대 반도체 미래차 6세대 6G 이동통신 탄소중립 디지털 바이오 등 6개 분야 혁신 기술이 소개된다. 산업화 세션에서는 지속 가능 성장을 위한 ESG 나노융합기술을 주제로 초청 강연이 열린다. 나노제품거래상담회 전시회테크니컬투어 최신기술발표회 등 다양한 나노 관련 부대 행사도 준비된다.<|end_of_text|>\n",
      "\n",
      "Assistant: 국내 3대 나노행사 중 하나인 나노코리아 2022 가 6일부터 3일간 경기도 킨텍스 제1전시장 4·5홀에서 개최되는데, 삼성전자 등 주요 기업과 나노 기술 기업 등 총 360개사가 참여할 예정이며 나노기술과 산업의 현재 미래 트랜드를 조망하는 기조 강연을 시작으로 나노 융합 전시화와 국제 심포지엄 행사가 다양하게 개최된다.<|end_of_text|>\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "기준금리 첫 0.5%P 인상 관측 두 달 연속 ‘빅스텝’ 단행 가능성 유동성 줄여도 인플레 영향 제한 고강도 긴축에 서민 고통 커져 전문가 “취약층 부채감면 고려” 이복현 ‘이자 장사’ 경고에… 예적금 금리 속속 인상 이복현 금융감독원장의 ‘이자 장사’ 경고가 나온 이후 시중은행들이 대출 금리 인하와 예적금 금리 인상에 나선 가운데 4일 경기 수원에서 한 여성이 폐지를 잔뜩 실은 손수레를 끌고 은행 금리 안내문이 외벽에 붙은 공사장 앞을 지나고 있다.뉴스1 지난달 소비자물가 상승률이 6%대에 달할 것이란 관측이 이어지면서 한국은행이 기준금리를 한 번에 0.5% 포인트 올리는 ‘빅스텝’을 사상 처음으로 단행할 것이란 전망이 지배적이다. 이달과 다음달 두 달 연속 빅스텝을 밟을 가능성도 제기된다. 한은이 긴축의 고삐를 바짝 조이는 것은 물가 안정이란 책무를 이행하기 위한 것이지만 효과가 크지 않을 것이란 전망도 많다. 지금의 인플레이션은 공급과 대외적 요인이 주요 원인으로 작용하고 있기 때문이다. 금리 인상이 물가는 잡지 못하면서 가계와 기업 등 경제주체에 대한 고통만 가중시키는 것 아니냐는 우려도 있다. MobileAdNew center 양준석 가톨릭대 경제학과 교수는 4일 “빅스텝을 단행해도 물가를 안정시키는 효과는 거의 없을 것으로 본다”며 “금리를 올리고 유동성을 줄이면 물가를 자극하는 수요를 일부 억제할 순 있지만 가장 큰 인플레이션 야기 요인인 대외적 요인엔 영향을 끼치지 못한다”고 말했다. 양 교수는 그럼에도 빅스텝은 필요하다고 덧붙였는데 한미 금리 역전에 따른 외국인 자본 이탈이 우려되기 때문이다. 지나치게 강도 높은 긴축은 경제주체를 고통에 빠뜨리고 경기를 침체시키는 등 부작용을 초래할 수 있다는 지적도 나온다. ‘인플레이션 파이터’ 폴 볼커 전 미 연방준비제도이사회 연준 의장은 스태그플레이션 경기침체 속 물가 상승 이 극성을 부리던 1979년 11.5%였던 기준금리를 1981년 20%까지 끌어올렸다. 이런 조치는 물가를 잡는 데 성공했지만 기업이 줄도산하고 실업률이 11%까지 치솟는 등 미국 경기를 냉각기에 빠뜨렸다. 볼커의 조치가 효과적이었는지는 지금도 논쟁 대상이다. 한국개발연구원 KDI 도 최근 보고서에서 고물가로 인해 기준금리 인상이 필요하지만 미국 금리 인상 속도를 따라잡기 위해 과도하게 높일 필요는 없다고 제언했다. 정규철 경제전망실장은 “미국을 따라 금리를 인상하면 경기 둔화가 그대로 파급되지만 독립적인 통화정책을 수행할 경우엔 일시적인 물가상승 외엔 큰 영향을 받지 않는다”며 “한미 기준금리 격차는 용인할 필요가 있다”고 했다. 박창균 자본시장연구원 연구조정실장은 “‘공짜 점심은 없다’는 경제학 격언처럼 물가를 안정시키려면 고통스럽더라도 강도 높은 긴축을 견뎌야 한다”며 “다만 가파른 금리 인상을 버티기 힘든 자영업자와 저소득층 등 취약계층을 보듬는 정책적 노력을 정부가 펼쳐야 한다”고 제언했다. 재정을 풀어 이들을 지원하거나 부채 감면 등의 조치를 취해야 한다는 것이다. 앞서 이창용 한은 총재는 “빅스텝은 물가 하나만 보고 결정하는 게 아니다”라며 “물가가 올랐을 때 우리 경기나 환율에 미치는 영향도 봐야 한다”고 말했다.<|end_of_text|>\n",
      "\n",
      "Assistant: 한국은행이 기준금리를 한 번에 0.5% 포인트 올리는 ‘빅스텝’을 사상 처음으로 단행할 것이란 전망에 대해 MobileAdNew center 양준석 가톨릭대 경제학과 교수는 데이터를 올리고 유동성을 줄이면 물가를 자극하는 수요를 일부 억제할 순 있지만 가장 큰 인플레이션 야기 요인인 대외적 요인엔 영향을 끼치지 못한다”고 말했으며 한국개발연구원 KDI 도 최근 보고서에서 고물가로 인해 기준금리 인상이 필요하지만 미국 금리 인상 속도를 따라잡기 위해 과도하게 높일 필요는 없다고 제언했다.<|end_of_text|>You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "기준금리 첫 0.5%P 인상 관측 두 달 연속 ‘빅스텝’ 단행 가능성 유동성 줄여도 인플레 영향 제한 고강도 긴축에 서민 고통 커져 전문가 “취약층 부채감면 고려” 이복현 ‘이자 장사’ 경고에… 예적금 금리 속속 인상 이복현 금융감독원장의 ‘이자 장사’ 경고가 나온 이후 시중은행들이 대출 금리 인하와 예적금 금리 인상에 나선 가운데 4일 경기 수원에서 한 여성이 폐지를 잔뜩 실은 손수레를 끌고 은행 금리 안내문이 외벽에 붙은 공사장 앞을 지나고 있다.뉴스1 지난달 소비자물가 상승률이 6%대에 달할 것이란 관측이 이어지면서 한국은행이 기준금리를 한 번에 0.5% 포인트 올리는 ‘빅스텝’을 사상 처음으로 단행할 것이란 전망이 지배적이다. 이달과 다음달 두 달 연속 빅스텝을 밟을 가능성도 제기된다. 한은이 긴축의 고삐를 바짝 조이는 것은 물가 안정이란 책무를 이행하기 위한 것이지만 효과가 크지 않을 것이란 전망도 많다. 지금의 인플레이션은 공급과 대외적 요인이 주요 원인으로 작용하고 있기 때문이다. 금리 인상이 물가는 잡지 못하면서 가계와 기업 등 경제주체에 대한 고통만 가중시키는 것 아니냐는 우려도 있다. MobileAdNew center 양준석 가톨릭대 경제학과 교수는 4일 “빅스텝을 단행해도 물가를 안정시키는 효과는 거의 없을 것으로 본다”며 “금리를 올리고 유동성을 줄이면 물가를 자극하는 수요를 일부 억제할 순 있지만 가장 큰 인플레이션 야기 요인인 대외적 요인엔 영향을 끼치지 못한다”고 말했다. 양 교수는 그럼에도 빅스텝은 필요하다고 덧붙였는데 한미 금리 역전에 따른 외국인 자본 이탈이 우려되기 때문이다. 지나치게 강도 높은 긴축은 경제주체를 고통에 빠뜨리고 경기를 침체시키는 등 부작용을 초래할 수 있다는 지적도 나온다. ‘인플레이션 파이터’ 폴 볼커 전 미 연방준비제도이사회 연준 의장은 스태그플레이션 경기침체 속 물가 상승 이 극성을 부리던 1979년 11.5%였던 기준금리를 1981년 20%까지 끌어올렸다. 이런 조치는 물가를 잡는 데 성공했지만 기업이 줄도산하고 실업률이 11%까지 치솟는 등 미국 경기를 냉각기에 빠뜨렸다. 볼커의 조치가 효과적이었는지는 지금도 논쟁 대상이다. 한국개발연구원 KDI 도 최근 보고서에서 고물가로 인해 기준금리 인상이 필요하지만 미국 금리 인상 속도를 따라잡기 위해 과도하게 높일 필요는 없다고 제언했다. 정규철 경제전망실장은 “미국을 따라 금리를 인상하면 경기 둔화가 그대로 파급되지만 독립적인 통화정책을 수행할 경우엔 일시적인 물가상승 외엔 큰 영향을 받지 않는다”며 “한미 기준금리 격차는 용인할 필요가 있다”고 했다. 박창균 자본시장연구원 연구조정실장은 “‘공짜 점심은 없다’는 경제학 격언처럼 물가를 안정시키려면 고통스럽더라도 강도 높은 긴축을 견뎌야 한다”며 “다만 가파른 금리 인상을 버티기 힘든 자영업자와 저소득층 등 취약계층을 보듬는 정책적 노력을 정부가 펼쳐야 한다”고 제언했다. 재정을 풀어 이들을 지원하거나 부채 감면 등의 조치를 취해야 한다는 것이다. 앞서 이창용 한은 총재는 “빅스텝은 물가 하나만 보고 결정하는 게 아니다”라며 “물가가 올랐을 때 우리 경기나 환율에 미치는 영향도 봐야 한다”고 말했다.<|end_of_text|>\n",
      "\n",
      "Assistant: 한국은행이 기준금리를 한 번에 0.5% 포인트 올리는 ‘빅스텝’을 사상 처음으로 단행할 것이란 전망에 대해 MobileAdNew center 양준석 가톨릭대 경제학과 교수는 데이터를 올리고 유동성을 줄이면 물가를 자극하는 수요를 일부 억제할 순 있지만 가장 큰 인플레이션 야기 요인인 대외적 요인엔 영향을 끼치지 못한다”고 말했으며 한국개발연구원 KDI 도 최근 보고서에서 고물가로 인해 기준금리 인상이 필요하지만 미국 금리 인상 속도를 따라잡기 위해 과도하게 높일 필요는 없다고 제언했다.<|end_of_text|>\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "기준금리 첫 0.5%P 인상 관측 두 달 연속 ‘빅스텝’ 단행 가능성 유동성 줄여도 인플레 영향 제한 고강도 긴축에 서민 고통 커져 전문가 “취약층 부채감면 고려” 이복현 ‘이자 장사’ 경고에… 예적금 금리 속속 인상 이복현 금융감독원장의 ‘이자 장사’ 경고가 나온 이후 시중은행들이 대출 금리 인하와 예적금 금리 인상에 나선 가운데 4일 경기 수원에서 한 여성이 폐지를 잔뜩 실은 손수레를 끌고 은행 금리 안내문이 외벽에 붙은 공사장 앞을 지나고 있다.뉴스1 지난달 소비자물가 상승률이 6%대에 달할 것이란 관측이 이어지면서 한국은행이 기준금리를 한 번에 0.5% 포인트 올리는 ‘빅스텝’을 사상 처음으로 단행할 것이란 전망이 지배적이다. 이달과 다음달 두 달 연속 빅스텝을 밟을 가능성도 제기된다. 한은이 긴축의 고삐를 바짝 조이는 것은 물가 안정이란 책무를 이행하기 위한 것이지만 효과가 크지 않을 것이란 전망도 많다. 지금의 인플레이션은 공급과 대외적 요인이 주요 원인으로 작용하고 있기 때문이다. 금리 인상이 물가는 잡지 못하면서 가계와 기업 등 경제주체에 대한 고통만 가중시키는 것 아니냐는 우려도 있다. MobileAdNew center 양준석 가톨릭대 경제학과 교수는 4일 “빅스텝을 단행해도 물가를 안정시키는 효과는 거의 없을 것으로 본다”며 “금리를 올리고 유동성을 줄이면 물가를 자극하는 수요를 일부 억제할 순 있지만 가장 큰 인플레이션 야기 요인인 대외적 요인엔 영향을 끼치지 못한다”고 말했다. 양 교수는 그럼에도 빅스텝은 필요하다고 덧붙였는데 한미 금리 역전에 따른 외국인 자본 이탈이 우려되기 때문이다. 지나치게 강도 높은 긴축은 경제주체를 고통에 빠뜨리고 경기를 침체시키는 등 부작용을 초래할 수 있다는 지적도 나온다. ‘인플레이션 파이터’ 폴 볼커 전 미 연방준비제도이사회 연준 의장은 스태그플레이션 경기침체 속 물가 상승 이 극성을 부리던 1979년 11.5%였던 기준금리를 1981년 20%까지 끌어올렸다. 이런 조치는 물가를 잡는 데 성공했지만 기업이 줄도산하고 실업률이 11%까지 치솟는 등 미국 경기를 냉각기에 빠뜨렸다. 볼커의 조치가 효과적이었는지는 지금도 논쟁 대상이다. 한국개발연구원 KDI 도 최근 보고서에서 고물가로 인해 기준금리 인상이 필요하지만 미국 금리 인상 속도를 따라잡기 위해 과도하게 높일 필요는 없다고 제언했다. 정규철 경제전망실장은 “미국을 따라 금리를 인상하면 경기 둔화가 그대로 파급되지만 독립적인 통화정책을 수행할 경우엔 일시적인 물가상승 외엔 큰 영향을 받지 않는다”며 “한미 기준금리 격차는 용인할 필요가 있다”고 했다. 박창균 자본시장연구원 연구조정실장은 “‘공짜 점심은 없다’는 경제학 격언처럼 물가를 안정시키려면 고통스럽더라도 강도 높은 긴축을 견뎌야 한다”며 “다만 가파른 금리 인상을 버티기 힘든 자영업자와 저소득층 등 취약계층을 보듬는 정책적 노력을 정부가 펼쳐야 한다”고 제언했다. 재정을 풀어 이들을 지원하거나 부채 감면 등의 조치를 취해야 한다는 것이다. 앞서 이창용 한은 총재는 “빅스텝은 물가 하나만 보고 결정하는 게 아니다”라며 “물가가 올랐을 때 우리 경기나 환율에 미치는 영향도 봐야 한다”고 말했다.<|end_of_text|>\n",
      "\n",
      "Assistant: 한국은행이 기준금리를 한 번에 0.5% 포인트 올리는 ‘빅스텝’을 사상 처음으로 단행할 것이란 전망에 대해 MobileAdNew center 양준석 가톨릭대 경제학과 교수는 데이터를 올리고 유동성을 줄이면 물가를 자극하는 수요를 일부 억제할 순 있지만 가장 큰 인플레이션 야기 요인인 대외적 요인엔 영향을 끼치지 못한다”고 말했으며 한국개발연구원 KDI 도 최근 보고서에서 고물가로 인해 기준금리 인상이 필요하지만 미국 금리 인상 속도를 따라잡기 위해 과도하게 높일 필요는 없다고 제언했다.<|end_of_text|>\n",
      "\n",
      "Downloading shards: 100%|██████████████████████| 4/4 [00:00<00:00, 17015.43it/s]\n",
      "Downloading shards: 100%|██████████████████████| 4/4 [00:00<00:00, 17260.51it/s]\n",
      "Downloading shards: 100%|██████████████████████| 4/4 [00:00<00:00, 11140.25it/s]\n",
      "Downloading shards: 100%|██████████████████████| 4/4 [00:00<00:00, 15592.21it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.27s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.28s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:05<00:00,  1.29s/it]\n",
      "Generating train split: 148 examples [00:00, 1985.70 examples/s]\n",
      "Generating train split: 153 examples [00:00, 3700.15 examples/s]\n",
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n",
      "{'loss': 2.2586, 'grad_norm': 0.37890625, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "100%|███████████████████████████████████████████| 18/18 [02:15<00:00,  7.36s/it]\n",
      "  0%|                                                    | 0/39 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|██▎                                         | 2/39 [00:01<00:20,  1.82it/s]\u001b[A\n",
      "  8%|███▍                                        | 3/39 [00:02<00:28,  1.28it/s]\u001b[A\n",
      " 10%|████▌                                       | 4/39 [00:03<00:31,  1.11it/s]\u001b[A\n",
      " 13%|█████▋                                      | 5/39 [00:04<00:32,  1.03it/s]\u001b[A\n",
      " 15%|██████▊                                     | 6/39 [00:05<00:33,  1.01s/it]\u001b[A\n",
      " 18%|███████▉                                    | 7/39 [00:06<00:33,  1.04s/it]\u001b[A\n",
      " 21%|█████████                                   | 8/39 [00:07<00:32,  1.06s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 9/39 [00:08<00:32,  1.07s/it]\u001b[A\n",
      " 26%|███████████                                | 10/39 [00:09<00:31,  1.08s/it]\u001b[A\n",
      " 28%|████████████▏                              | 11/39 [00:11<00:30,  1.09s/it]\u001b[A\n",
      " 31%|█████████████▏                             | 12/39 [00:12<00:29,  1.09s/it]\u001b[A\n",
      " 33%|██████████████▎                            | 13/39 [00:13<00:28,  1.10s/it]\u001b[A\n",
      " 36%|███████████████▍                           | 14/39 [00:14<00:27,  1.10s/it]\u001b[A\n",
      " 38%|████████████████▌                          | 15/39 [00:15<00:26,  1.10s/it]\u001b[A\n",
      " 41%|█████████████████▋                         | 16/39 [00:16<00:25,  1.10s/it]\u001b[A\n",
      " 44%|██████████████████▋                        | 17/39 [00:17<00:24,  1.10s/it]\u001b[A\n",
      " 46%|███████████████████▊                       | 18/39 [00:18<00:23,  1.10s/it]\u001b[A\n",
      " 49%|████████████████████▉                      | 19/39 [00:19<00:22,  1.10s/it]\u001b[A\n",
      " 51%|██████████████████████                     | 20/39 [00:20<00:20,  1.10s/it]\u001b[A\n",
      " 54%|███████████████████████▏                   | 21/39 [00:22<00:19,  1.10s/it]\u001b[A\n",
      " 56%|████████████████████████▎                  | 22/39 [00:23<00:18,  1.10s/it]\u001b[A\n",
      " 59%|█████████████████████████▎                 | 23/39 [00:24<00:17,  1.11s/it]\u001b[A\n",
      " 62%|██████████████████████████▍                | 24/39 [00:25<00:16,  1.11s/it]\u001b[A\n",
      " 64%|███████████████████████████▌               | 25/39 [00:26<00:15,  1.10s/it]\u001b[A\n",
      " 67%|████████████████████████████▋              | 26/39 [00:27<00:14,  1.10s/it]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 27/39 [00:28<00:13,  1.11s/it]\u001b[A\n",
      " 72%|██████████████████████████████▊            | 28/39 [00:29<00:12,  1.11s/it]\u001b[A\n",
      " 74%|███████████████████████████████▉           | 29/39 [00:30<00:11,  1.11s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 30/39 [00:32<00:09,  1.11s/it]\u001b[A\n",
      " 79%|██████████████████████████████████▏        | 31/39 [00:33<00:08,  1.11s/it]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 32/39 [00:34<00:07,  1.11s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 33/39 [00:35<00:06,  1.11s/it]\u001b[A\n",
      " 87%|█████████████████████████████████████▍     | 34/39 [00:36<00:05,  1.11s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████▌    | 35/39 [00:37<00:04,  1.11s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 36/39 [00:38<00:03,  1.11s/it]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 37/39 [00:39<00:02,  1.11s/it]\u001b[A\n",
      " 97%|█████████████████████████████████████████▉ | 38/39 [00:40<00:01,  1.11s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 2.0421462059020996, 'eval_runtime': 43.431, 'eval_samples_per_second': 3.523, 'eval_steps_per_second': 0.898, 'epoch': 0.97}\n",
      "100%|███████████████████████████████████████████| 18/18 [02:59<00:00,  7.36s/it]\n",
      "100%|███████████████████████████████████████████| 39/39 [00:42<00:00,  1.11s/it]\u001b[A\n",
      "                                                                                \u001b[A[rank0]:[2024-06-29 15:11:21,172] torch.distributed.fsdp._debug_utils: [WARNING] FSDP _optim_state_dict() profiling:  defaultdict(<class 'float'>, {'preprocessing': 0.23867278899706434, 'preprocessing_with_comm': 0.0031676530052209273, 'state_converting': 0.21232054100255482, <Type.ALL: 'all'>: 0.4652380580082536})\n",
      "{'train_runtime': 193.6273, 'train_samples_per_second': 0.764, 'train_steps_per_second': 0.093, 'train_loss': 2.1801940070258246, 'epoch': 0.97}\n",
      "100%|███████████████████████████████████████████| 18/18 [03:13<00:00, 10.76s/it]\n"
     ]
    }
   ],
   "source": [
    "!ACCELERATE_USE_FSDP=1 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=4 \\\n",
    "../scripts/local_run_fsdp_qlora.py \\\n",
    "--config local_llama_3_8b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441607e4-f93f-438e-8985-99a76233fe47",
   "metadata": {},
   "source": [
    "## 4. 베이스 모델과 훈련된 모델 머지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cebdb212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('meta-llama/Meta-Llama-3-8B',\n",
       " '/home/ec2-user/SageMaker/models/llama-3-8b-naver-news')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id, output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8acc4",
   "metadata": {},
   "source": [
    "### 모델 머지 및 로컬에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8136.38it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.94it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load PEFT model on CPU\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    output_dir,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")  \n",
    "# Merge LoRA and base model and save\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(output_dir,safe_serialization=True, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0ea42",
   "metadata": {},
   "source": [
    "### 머지된 모델 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a6607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 4/4 [00:00<00:00, 8148.24it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.32s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  pretrained_model_name_or_path = output_dir,\n",
    "  torch_dtype=torch.float16,\n",
    "  quantization_config= {\"load_in_4bit\": True},\n",
    "  device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908046b",
   "metadata": {},
   "source": [
    "## 5. 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5388b34",
   "metadata": {},
   "source": [
    "### 테스트 데이터 셋 로딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15b47b",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acba2530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bcefe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddaf71a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m messages \u001b[38;5;241m=\u001b[39m test_dataset[rand_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     12\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages,add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m response \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m][input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(f\"**Query:**\\n{test_dataset[rand_idx]['messages'][1]['content']}\\n\")\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/peft/peft_model.py:1190\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_peft_forward_hooks(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1189\u001b[0m         kwargs \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1190\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1192\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/generation/utils.py:1622\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1614\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1615\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1616\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1617\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1618\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1619\u001b[0m     )\n\u001b[1;32m   1621\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1622\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample(\n\u001b[1;32m   1623\u001b[0m         input_ids,\n\u001b[1;32m   1624\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mprepared_logits_processor,\n\u001b[1;32m   1625\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1626\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mprepared_stopping_criteria,\n\u001b[1;32m   1627\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1628\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1629\u001b[0m         output_logits\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_logits,\n\u001b[1;32m   1630\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1631\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1632\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1633\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1634\u001b[0m     )\n\u001b[1;32m   1636\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1637\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1639\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1640\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1646\u001b[0m     )\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/generation/utils.py:2791\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2788\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2790\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2791\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2792\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2793\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2794\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2795\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2796\u001b[0m )\n\u001b[1;32m   2798\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2799\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1208\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1205\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1207\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1208\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1209\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1210\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1211\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1212\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1213\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1214\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1215\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1216\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1217\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1218\u001b[0m     cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1219\u001b[0m )\n\u001b[1;32m   1221\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1018\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1008\u001b[0m         decoder_layer\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m   1009\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         cache_position,\n\u001b[1;32m   1016\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1018\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   1019\u001b[0m         hidden_states,\n\u001b[1;32m   1020\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mcausal_mask,\n\u001b[1;32m   1021\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1022\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1023\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1024\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1025\u001b[0m         cache_position\u001b[39m=\u001b[39;49mcache_position,\n\u001b[1;32m   1026\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:756\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    755\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 756\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    757\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    759\u001b[0m outputs \u001b[39m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:240\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    238\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(down_proj)\n\u001b[1;32m    239\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 240\u001b[0m     down_proj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_proj(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgate_proj(x)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_proj(x))\n\u001b[1;32m    242\u001b[0m \u001b[39mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "from random import randint\n",
    "\n",
    "# Load our test dataset\n",
    "test_dataset = load_dataset(\"json\", data_files=test_data_json, split=\"train\")\n",
    "\n",
    "# Test on sample \n",
    "rand_idx = randint(0, len(test_dataset)-1)\n",
    "# messages = test_dataset[rand_idx][\"messages\"][:2]\n",
    "messages = test_dataset[rand_idx][\"text\"][:2]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages,add_generation_prompt=True,return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id= tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "\n",
    "# print(f\"**Query:**\\n{test_dataset[rand_idx]['messages'][1]['content']}\\n\")\n",
    "print(f\"**Query:**\\n{test_dataset[rand_idx]['text'][1]['content']}\\n\")\n",
    "print(f\"**Original Answer:**\\n{test_dataset[rand_idx]['text'][2]['content']}\\n\")\n",
    "print(f\"**Generated Answer:**\\n{tokenizer.decode(response,skip_special_tokens=True)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31a733d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llama3_puy310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
