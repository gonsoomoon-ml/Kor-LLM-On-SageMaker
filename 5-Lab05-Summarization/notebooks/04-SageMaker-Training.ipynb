{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 3 with PyTorch FSDP and Q-Lora on Amazon SageMaker\n",
    "\n",
    "This blog post walks you thorugh how to fine-tune a Llama 3 using PyTorch FSDP and Q-Lora with the help of Hugging Face [TRL](https://huggingface.co/docs/trl/index), [Transformers](https://huggingface.co/docs/transformers/index), [peft](https://huggingface.co/docs/peft/index) & [datasets](https://huggingface.co/docs/datasets/index) on Amazon SageMAker. In addition to FSDP we will use [Flash Attention v2](https://github.com/Dao-AILab/flash-attention) implementation. \n",
    "\n",
    "This blog is an extension and dedicated version to my [Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora](https://www.philschmid.de/fsdp-qlora-llama3) version, specifically tailored to run on Amazon SageMaker.\n",
    "\n",
    "1. [Setup development environment](#2-setup-development-environment)\n",
    "2. [Create and prepare the dataset](#3-create-and-prepare-the-dataset)\n",
    "3. [Fine-tune Llama 3 on Amazon SageMaker](#4-fine-tune-llm-using-trl-and-the-sfttrainer)\n",
    "4. [Deploy & Test fine-tuned Llama 3 on Amazon SageMaker](#5-test-and-evaluate-the-llm)\n",
    "\n",
    "_Note: This blog was created and validated on `ml.p4d.24xlarge` and `ml.g5.48.xlarge` instances. The configurations and code are optimized for `ml.p4d.24xlarge` with 8xA100 GPUs each with 40GB of Memory. We tried `ml.g5.12xlarge` but Amazon SageMaker reserves more memory than EC2. We plan to add support for `trn1` in the coming weeks._\n",
    "\n",
    "**FSDP + Q-Lora Background**\n",
    "\n",
    "In a collaboration between [Answer.AI](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html), Tim Dettmers [Q-Lora creator](https://github.com/TimDettmers/bitsandbytes) and [Hugging Face](https://huggingface.co/), we are proud to announce to share the support of Q-Lora and PyTorch FSDP (Fully Sharded Data Parallel). FSDP and Q-Lora allows you now to fine-tune Llama 2 70b or Mixtral 8x7B on 2x consumer GPUs (24GB). If you want to learn more about the background of this collaboration take a look at [You can now train a 70b language model at home](https://www.answer.ai/posts/2024-03-06-fsdp-qlora.html). Hugging Face PEFT is were the magic happens for this happens, read more about it in the [PEFT documentation](https://huggingface.co/docs/peft/v0.10.0/en/accelerate/fsdp).\n",
    "\n",
    "* [PyTorch FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) is a data/model parallelism technique that shards model across GPUs, reducing memory requirements and enabling the training of larger models more efficiently​​​​​​.\n",
    "* Q-LoRA is a fine-tuning method that leverages quantization and Low-Rank Adapters to efficiently reduced computational requirements and memory footprint. \n",
    "\n",
    "This blog post walks you thorugh how to fine-tune open LLMs from Hugging Face using Amazon SageMaker. This blog is an extension and dedicated version to my [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl) version, specifically tailored to run on Amazon SageMaker.\n",
    "\n",
    "\n",
    "\n",
    "## 1. Setup Development Environment\n",
    "\n",
    "Our first step is to install Hugging Face Libraries we need on the client to correctly prepare our dataset and start our training/evaluations jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers \"datasets[s3]==2.18.0\" \"sagemaker>=2.190.0\" \"huggingface_hub[cli]\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to login into Hugging Face to access the Llama 3 70b model and store our trained model on Hugging Face. If you don't have an account yet and accepted the terms, you can create one [here](https://huggingface.co/join). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli login --token YOUR_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::057716757052:role/dt2gsmoon\n",
      "sagemaker bucket: sagemaker-us-east-1-057716757052\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and prepare the dataset\n",
    "\n",
    "\n",
    "After our environment is set up, we can start creating and preparing our dataset. A fine-tuning dataset should have a diverse set of demonstrations of the task you want to solve. If you want to learn more about how to create a dataset, take a look at the [How to Fine-Tune LLMs in 2024 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2024-with-trl#3-create-and-prepare-the-dataset).\n",
    "\n",
    "We will use the [HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset a high-quality dataset of 10,000 instructions and demonstrations created by skilled human annotators. This data can be used for supervised fine-tuning (SFT) to make language models follow instructions better. No Robots was modelled after the instruction dataset described in OpenAI's [InstructGPT paper](https://huggingface.co/papers/2203.02155), and is comprised mostly of single-turn instructions.\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "```\n",
    "\n",
    "The [no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots) dataset has 10,000 split into 9,500 training and  500 test examples. Some samples are not including a `system` message. We will load the dataset with the `datasets` library, add a missing `system` message and save them to separate json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cf6ec89eb64569a2169227627ccc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 10.5M/10.5M [00:00<00:00, 144MB/s]\n",
      "Downloading data: 100%|██████████| 571k/571k [00:00<00:00, 13.7MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f9c5720f045668df69694aa1d534c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38c3abaf40247e79e6805b57c106a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9338aa7f69754873b29fd84bf65eca17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aea28d4fd184d8bbf12177f83c1f074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99625b7bb1a04c079fde6360356c5b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341fed694a974b5a8e3411e72e1ddc94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "      sample[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}] + sample[\"messages\"]\n",
    "      return sample\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/no_robots\")\n",
    "\n",
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "dataset = dataset.map(create_conversation, remove_columns=columns_to_remove,batched=False)\n",
    "\n",
    "# Filter out conversations which are corrupted with wrong turns, keep which have even number of turns after adding system message\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we processed the datasets we are going to use the [FileSystem integration](https://huggingface.co/docs/datasets/filesystems) to upload our dataset to S3. We are using the `sess.default_bucket()`, adjust this if you want to store the dataset in a different S3 bucket. We will use the S3 path later in our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/fsspec/registry.py:273: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c752956f54f441d84c1b0ae60bead9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8f43100b98408db26d084f071ecfe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/llama3/train/dataset.json\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/llama3/test/dataset.json\n",
      "https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-057716757052/?region=us-east-1&prefix=datasets/llama3/\n"
     ]
    }
   ],
   "source": [
    "# save train_dataset to s3 using our SageMaker session\n",
    "input_path = f's3://{sess.default_bucket()}/datasets/llama3'\n",
    "\n",
    "# save datasets to s3\n",
    "dataset[\"train\"].to_json(f\"{input_path}/train/dataset.json\", orient=\"records\")\n",
    "train_dataset_s3_path = f\"{input_path}/train/dataset.json\"\n",
    "dataset[\"test\"].to_json(f\"{input_path}/test/dataset.json\", orient=\"records\")\n",
    "test_dataset_s3_path = f\"{input_path}/test/dataset.json\"\n",
    "\n",
    "print(f\"Training data uploaded to:\")\n",
    "print(train_dataset_s3_path)\n",
    "print(test_dataset_s3_path)\n",
    "print(f\"https://s3.console.aws.amazon.com/s3/buckets/{sess.default_bucket()}/?region={sess.boto_region_name}&prefix={input_path.split('/', 3)[-1]}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tune Llama 3 on Amazon SageMaker\n",
    "\n",
    "We are now ready to fine-tune our model. We will use the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs. The `SFTTrainer` is a subclass of the `Trainer` from the `transformers`. We prepared a script [run_fsdp_qlora.py](../scripts/fsdp/run_fsdp_qlora.py) which will loads the dataset from disk, prepare the model, tokenizer and start the training. It usees the [SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs supporting:\n",
    "* Dataset formatting, including conversational and instruction format (✅ used)\n",
    "* Training on completions only, ignoring prompts (❌ not used)\n",
    "* Packing datasets for more efficient training (✅ used)\n",
    "* PEFT (parameter-efficient fine-tuning) support including Q-LoRA (✅ used)\n",
    "* Preparing the model and tokenizer for conversational fine-tuning (❌ not used, see below)\n",
    "\n",
    "For configuration we use the new `TrlParser`, that allows us to provide hyperparameters in a yaml file. This `yaml` will be uploaded and provided to Amazon SageMaker similar to our datasets. Below is the config file for fine-tuning Llama 3 70B on 8x A100 GPUs or 4x24GB GPUs. We are saving the config file as `fsdp_qlora_llama3_70b.yaml` and upload it to S3.\n",
    "\n",
    "For the chat template we use the Anthropic/Vicuna template, not the official one. Since we then would need to train and save the embedding layer as well leading to more memory requirements. If you wnat to use the official Llama 3 template comment in the `LLAMA_3_CHAT_TEMPLATE` in the `run_fsdp_qlora.py` script and make sure to add modules_to_save. The template used will look like this.\n",
    "\n",
    "```\n",
    "You are a helpful Assistant. \n",
    "\n",
    "Human: What is 2+2? \n",
    "\n",
    "Assistant: 2+2 equals 4.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llama_3_70b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile llama_3_70b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Meta-Llama-3-70b\"# Hugging Face model id\n",
    "max_seq_len:  3072 # 2048              # max sequence length for model and packing of the dataset\n",
    "# sagemaker specific parameters\n",
    "train_dataset_path: \"/opt/ml/input/data/train/\" # path to where SageMaker saves train dataset\n",
    "test_dataset_path: \"/opt/ml/input/data/test/\"   # path to where SageMaker saves test dataset\n",
    "# output_dir: \"/opt/ml/model\"            # path to where SageMaker will upload the model \n",
    "output_dir: \"/tmp/llama3\"            # path to where SageMaker will upload the model \n",
    "# training parameters\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 2                    # number of training epochs\n",
    "per_device_train_batch_size: 8         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "  backward_prefetch: \"backward_pre\"\n",
    "  forward_prefetch: \"false\"\n",
    "  use_orig_params: \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets upload the config file to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training config uploaded to:\n",
      "s3://sagemaker-us-east-1-057716757052/datasets/llama3/config/llama_3_70b_fsdp_qlora.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload the model yaml file to s3\n",
    "model_yaml = \"llama_3_70b_fsdp_qlora.yaml\"\n",
    "train_config_s3_path = S3Uploader.upload(local_path=model_yaml, desired_s3_uri=f\"{input_path}/config\")\n",
    "\n",
    "print(f\"Training config uploaded to:\")\n",
    "print(train_config_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. The Estimator manages the infrastructure use. Amazon SagMaker takes care of starting and managing all the required ec2 instances for us, provides the correct huggingface container, uploads the provided scripts and downloads the data from our S3 bucket into the container at `/opt/ml/input/data`. Then, it starts the training job by running.\n",
    "\n",
    "> Note: Make sure that you include the `requirements.txt` in the `source_dir` if you are using a custom training script. We recommend to just clone the whole repository.\n",
    "\n",
    "To use `torchrun` to execute our scripts, we only have to define the `distribution` parameter in our Estimator and set it to `{\"torch_distributed\": {\"enabled\": True}}`. This tells SageMaker to launch our training job with.\n",
    "\n",
    "```python\n",
    "torchrun --nnodes 2 --nproc_per_node 8 --master_addr algo-1 --master_port 7777 --node_rank 1 run_fsdp_qlora.py --config /opt/ml/input/data/config/config.yaml\n",
    "```\n",
    "\n",
    "The `HuggingFace` configuration below will start a training job on 1x `p4d.24xlarge` using 8x A100 GPUs. The amazing part about SageMaker is that you can easily scale up to 2x `p4d.24xlarge` by modifying the `instance_count`. SageMaker will take care of the rest for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'llama3-70b-exp1'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_fsdp_qlora.py',      # train script\n",
    "    source_dir           = '../scripts/fsdp',  # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.p4d.24xlarge',  # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    max_run              = 2*24*60*60,        # maximum runtime in seconds (days * hours * minutes * seconds)\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 500,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.36.0',          # the transformers version used in the training job\n",
    "    pytorch_version      = '2.1.0',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  {\n",
    "        \"config\": \"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\" # path to TRL config which was uploaded to s3\n",
    "    },\n",
    "    disable_output_compression = True,        # not compress output to save training time and cost\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},   # enables torchrun\n",
    "    environment  = {\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\", # set env variable to cache models in /tmp\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),       # huggingface token to access gated models, e.g. llama 3\n",
    "        \"ACCELERATE_USE_FSDP\": \"1\",             # enable FSDP\n",
    "        \"FSDP_CPU_RAM_EFFICIENT_LOADING\": \"1\"   # enable CPU RAM efficient loading\n",
    "    }, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: When using QLoRA, we only train adapters and not the full model. The [run_fsdp_qlora.py](../scripts/fsdp/run_fsdp_qlora.py) merges the `base_model`with the `adapter` at the end of the training to directly be able to deploy to Amazon SageMaker._\n",
    "\n",
    "We can now start our training job, with the `.fit()` method passing our S3 path to the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama3-70b-exp1-2024-06-25-12-39-12-572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 12:39:13 Starting - Starting the training job...\n",
      "2024-06-25 12:39:13 Pending - Training job waiting for capacity......\n",
      "2024-06-25 12:40:15 Pending - Preparing the instances for training...........................\n",
      "2024-06-25 12:45:06 Downloading - Downloading input data...\n",
      "2024-06-25 12:45:41 Downloading - Downloading the training image...............\n",
      "2024-06-25 12:48:02 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:05,224 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:05,315 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:05,324 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:05,325 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:05,325 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:06,725 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.40.0 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.6/137.6 kB 5.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.18.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.29.3 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate==0.4.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.4.1)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.43.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting huggingface_hub==0.22.2 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl==0.8.6 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.8.6-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.10.0 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (3.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.20,>=0.19 (from transformers==4.40.0->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (0.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.0->-r requirements.txt (line 1)) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (15.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0->-r requirements.txt (line 2)) (2023.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.18.0->-r requirements.txt (line 2)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.3->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.1->-r requirements.txt (line 4)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub==0.22.2->-r requirements.txt (line 6)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tyro>=0.5.11 in /opt/conda/lib/python3.10/site-packages (from trl==0.8.6->-r requirements.txt (line 7)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.18.0->-r requirements.txt (line 2)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.40.0->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (0.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (13.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shtab>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (1.6.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.18.0->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0->-r requirements.txt (line 2)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (2.16.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.29.3->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6->-r requirements.txt (line 7)) (0.1.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.0/9.0 MB 82.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 510.5/510.5 kB 55.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.29.3-py3-none-any.whl (297 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.6/297.6 kB 38.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.8/119.8 MB 19.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 39.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.8.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.2/245.2 kB 33.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.10.0-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 31.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 106.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: huggingface_hub, tokenizers, bitsandbytes, accelerate, transformers, datasets, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface_hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.15.1\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.15.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.15.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.42.0\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.42.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.42.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.25.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.25.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.25.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.36.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.36.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.36.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.15.0\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.15.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.15.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.7.4\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.7.4:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.7.4\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.7.1\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.7.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.29.3 bitsandbytes-0.43.1 datasets-2.18.0 huggingface_hub-0.22.2 peft-0.10.0 tokenizers-0.19.1 transformers-4.40.0 trl-0.8.6\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.3.2 -> 24.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:20,374 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:20,374 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:20,499 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:20,601 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:20,610 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:20,701 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-06-25 12:49:20,712 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"config\": \"/opt/ml/input/data/config\",\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"config\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"llama3-70b-exp1-2024-06-25-12-39-12-572\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-057716757052/llama3-70b-exp1-2024-06-25-12-39-12-572/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_fsdp_qlora\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_fsdp_qlora.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_fsdp_qlora.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"config\",\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_fsdp_qlora\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-057716757052/llama3-70b-exp1-2024-06-25-12-39-12-572/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"config\":\"/opt/ml/input/data/config\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"config\":\"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"config\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"llama3-70b-exp1-2024-06-25-12-39-12-572\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-057716757052/llama3-70b-exp1-2024-06-25-12-39-12-572/source/sourcedir.tar.gz\",\"module_name\":\"run_fsdp_qlora\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_fsdp_qlora.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CONFIG=/opt/ml/input/data/config\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=/opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 8 run_fsdp_qlora.py --config /opt/ml/input/data/config/llama_3_70b_fsdp_qlora.yaml\u001b[0m\n",
      "\u001b[34m[2024-06-25 12:49:21,972] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-06-25 12:49:21,972] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-06-25 12:49:21,972] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-06-25 12:49:21,972] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 9485 examples [00:00, 109501.76 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 500 examples [00:00, 112641.10 examples/s]\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9485 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9485 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9485 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9485 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9485 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9485 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9485 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/9485 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 1000/9485 [00:00<00:00, 9656.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 1000/9485 [00:00<00:00, 9714.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 1000/9485 [00:00<00:00, 9669.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 1000/9485 [00:00<00:00, 9783.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 1000/9485 [00:00<00:00, 9746.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 1000/9485 [00:00<00:00, 9708.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 1000/9485 [00:00<00:00, 9747.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 907/9485 [00:00<00:00, 8997.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 2340/9485 [00:00<00:00, 11822.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 2331/9485 [00:00<00:00, 11798.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 2322/9485 [00:00<00:00, 11724.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 2343/9485 [00:00<00:00, 11905.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 2344/9485 [00:00<00:00, 11887.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 2323/9485 [00:00<00:00, 11749.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 2325/9485 [00:00<00:00, 11780.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 2000/9485 [00:00<00:00, 10058.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 3702/9485 [00:00<00:00, 12614.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 3700/9485 [00:00<00:00, 12581.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 3693/9485 [00:00<00:00, 12473.37 examples/s]#015Map:  39%|███▉      | 3707/9485 [00:00<00:00, 12667.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 3692/9485 [00:00<00:00, 12593.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 3696/9485 [00:00<00:00, 12523.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 3690/9485 [00:00<00:00, 12426.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 3167/9485 [00:00<00:00, 10784.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 5027/9485 [00:00<00:00, 12861.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 5012/9485 [00:00<00:00, 12787.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 5028/9485 [00:00<00:00, 12876.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 5000/9485 [00:00<00:00, 12679.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 5003/9485 [00:00<00:00, 12793.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 5000/9485 [00:00<00:00, 12712.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 5000/9485 [00:00<00:00, 12648.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 4337/9485 [00:00<00:00, 11141.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 6386/9485 [00:00<00:00, 13118.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 6365/9485 [00:00<00:00, 13051.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 6385/9485 [00:00<00:00, 13124.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 6336/9485 [00:00<00:00, 12917.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 6352/9485 [00:00<00:00, 13041.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 6343/9485 [00:00<00:00, 12964.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 6340/9485 [00:00<00:00, 12912.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 5514/9485 [00:00<00:00, 11364.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 7751/9485 [00:00<00:00, 13298.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████▏ | 7721/9485 [00:00<00:00, 13220.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 7745/9485 [00:00<00:00, 13284.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 7691/9485 [00:00<00:00, 13086.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 7690/9485 [00:00<00:00, 13150.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 7696/9485 [00:00<00:00, 13142.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 7694/9485 [00:00<00:00, 13093.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 6682/9485 [00:00<00:00, 11470.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 13244.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 12865.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 13175.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 12809.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 13228.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 12876.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 13061.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 12698.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 12975.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 13092.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 12719.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 12739.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 13028.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 12681.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 7853/9485 [00:00<00:00, 11545.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/500 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/500 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/500 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/500 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/500 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/500 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 11445.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 9485/9485 [00:00<00:00, 11183.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/500 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 500/500 [00:00<00:00, 12225.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 500/500 [00:00<00:00, 12162.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 500/500 [00:00<00:00, 12222.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 500/500 [00:00<00:00, 12127.24 examples/s]\u001b[0m\n",
      "\u001b[34mYou are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\u001b[0m\n",
      "\u001b[34mHuman: Make a list of 10 weird sights to see on a road trip in Colorado.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: 1. 12-foot-tall grizzly bear in Alamosa. \u001b[0m\n",
      "\u001b[34m2. The childhood home that you've seen in the movie, Indiana Jones and the Last Crusade in Antonito. \u001b[0m\n",
      "\u001b[34m3. In the town of Arvada, you can see a horse statue wearing a hazmat mask. \u001b[0m\n",
      "\u001b[34m4. Bailey, CO has a diner shaped like a hot dog. \u001b[0m\n",
      "\u001b[34m5. You won't believe it until you see it, but in the town of Fruita, there is a dinosaur riding a bicycle. \u001b[0m\n",
      "\u001b[34m6. Sasquatch Museum in Bailey. A must-see. \u001b[0m\n",
      "\u001b[34m7. Aspen is home to Ashcroft Ghost Town. \u001b[0m\n",
      "\u001b[34m8. The town of Aurora has a three-legged chair at the intersection of Colfax Ave. and Dallas St.\u001b[0m\n",
      "\u001b[34m9. There is a statue of a Papa Burger with root beer and burger in Berthoud. The statue stands on the roof of an A&W. \u001b[0m\n",
      "\u001b[34m10. A fake police officer sits in a patrol car, towards the north end of town in Alma. <|end_of_text|>\u001b[0m\n",
      "\u001b[34mYou are Llama, an AI assistant created by Philipp to be helpful and honest. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\u001b[0m\n",
      "\u001b[34mHuman: I am looking for healthy snacks for a road trip. They cannot contain any nuts, as I am deathly allergic. Please give me 3 options.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mAssistant: Here are three healthy road trips snacks that don't contain nuts:\u001b[0m\n",
      "\u001b[34m1. Apple chips: This healthy snack is a perfect snack for on the go, and you can mix it up by sprinkling cinnamon on top.\u001b[0m\n",
      "\u001b[34m2. Homemade trail mix: Trail mix doesn't have to contain nuts--make your own by using ingredients such as sunflower seeds, pumpkin seeds, dried fruit, and some dark chocolate chips.\u001b[0m\n",
      "\u001b[34m3. Fresh fruit and cheese: Pick your favorite fresh fruit and pair it with some slices of cheese.<|end_of_text|>\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 500/500 [00:00<00:00, 12141.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 500/500 [00:00<00:00, 12146.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 500/500 [00:00<00:00, 12087.89 examples/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.5+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:65:239 [1] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:69:237 [5] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:66:236 [2] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:64:235 [0] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:71:238 [7] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:70:242 [6] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:68:240 [4] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:67:241 [3] configure_nvls_option:287 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading shards:   3%|▎         | 1/30 [00:11<05:39, 11.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   3%|▎         | 1/30 [00:11<05:38, 11.66s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   3%|▎         | 1/30 [00:11<05:38, 11.67s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   3%|▎         | 1/30 [00:11<05:39, 11.69s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   3%|▎         | 1/30 [00:11<05:40, 11.73s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   3%|▎         | 1/30 [00:11<05:39, 11.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   3%|▎         | 1/30 [00:11<05:41, 11.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   3%|▎         | 1/30 [00:11<05:43, 11.84s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   7%|▋         | 2/30 [00:24<05:39, 12.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   7%|▋         | 2/30 [00:24<05:38, 12.09s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   7%|▋         | 2/30 [00:24<05:39, 12.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   7%|▋         | 2/30 [00:24<05:39, 12.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   7%|▋         | 2/30 [00:24<05:38, 12.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   7%|▋         | 2/30 [00:24<05:39, 12.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   7%|▋         | 2/30 [00:24<05:39, 12.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:   7%|▋         | 2/30 [00:24<05:39, 12.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  10%|█         | 3/30 [00:43<06:50, 15.22s/it]#015Downloading shards:  10%|█         | 3/30 [00:42<06:50, 15.21s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  10%|█         | 3/30 [00:43<06:51, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  10%|█         | 3/30 [00:43<06:51, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  10%|█         | 3/30 [00:43<06:50, 15.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  10%|█         | 3/30 [00:43<06:51, 15.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  10%|█         | 3/30 [00:43<06:51, 15.22s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  10%|█         | 3/30 [00:43<06:51, 15.23s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  13%|█▎        | 4/30 [00:52<05:37, 12.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  13%|█▎        | 4/30 [00:52<05:37, 12.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  13%|█▎        | 4/30 [00:52<05:37, 12.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  13%|█▎        | 4/30 [00:52<05:38, 13.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  13%|█▎        | 4/30 [00:52<05:37, 13.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  13%|█▎        | 4/30 [00:52<05:37, 13.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  13%|█▎        | 4/30 [00:52<05:38, 13.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  13%|█▎        | 4/30 [00:52<05:38, 13.03s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  17%|█▋        | 5/30 [01:02<04:54, 11.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  17%|█▋        | 5/30 [01:02<04:54, 11.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  17%|█▋        | 5/30 [01:02<04:54, 11.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  17%|█▋        | 5/30 [01:02<04:54, 11.79s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  17%|█▋        | 5/30 [01:02<04:54, 11.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  17%|█▋        | 5/30 [01:02<04:54, 11.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  17%|█▋        | 5/30 [01:02<04:54, 11.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  17%|█▋        | 5/30 [01:02<04:54, 11.80s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  20%|██        | 6/30 [01:11<04:23, 10.96s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  20%|██        | 6/30 [01:11<04:22, 10.95s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  20%|██        | 6/30 [01:11<04:22, 10.96s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  20%|██        | 6/30 [01:11<04:22, 10.96s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  20%|██        | 6/30 [01:11<04:22, 10.96s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  20%|██        | 6/30 [01:11<04:23, 10.97s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  20%|██        | 6/30 [01:11<04:23, 10.97s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  20%|██        | 6/30 [01:11<04:23, 10.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  23%|██▎       | 7/30 [01:20<03:59, 10.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  23%|██▎       | 7/30 [01:20<03:59, 10.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  23%|██▎       | 7/30 [01:20<03:59, 10.42s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  23%|██▎       | 7/30 [01:20<03:59, 10.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  23%|██▎       | 7/30 [01:20<03:59, 10.43s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  23%|██▎       | 7/30 [01:21<04:00, 10.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  23%|██▎       | 7/30 [01:20<04:00, 10.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  23%|██▎       | 7/30 [01:20<04:00, 10.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  27%|██▋       | 8/30 [01:38<04:39, 12.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  27%|██▋       | 8/30 [01:38<04:39, 12.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  27%|██▋       | 8/30 [01:38<04:39, 12.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  27%|██▋       | 8/30 [01:38<04:40, 12.73s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  27%|██▋       | 8/30 [01:38<04:39, 12.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  27%|██▋       | 8/30 [01:38<04:40, 12.73s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  27%|██▋       | 8/30 [01:38<04:40, 12.73s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  27%|██▋       | 8/30 [01:38<04:40, 12.73s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  30%|███       | 9/30 [01:57<05:08, 14.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  30%|███       | 9/30 [01:57<05:08, 14.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  30%|███       | 9/30 [01:57<05:09, 14.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  30%|███       | 9/30 [01:57<05:09, 14.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  30%|███       | 9/30 [01:57<05:08, 14.71s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  30%|███       | 9/30 [01:57<05:09, 14.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  30%|███       | 9/30 [01:57<05:09, 14.73s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  30%|███       | 9/30 [01:57<05:09, 14.72s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 10/30 [02:10<04:40, 14.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 10/30 [02:10<04:39, 14.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 10/30 [02:10<04:40, 14.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 10/30 [02:10<04:40, 14.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 10/30 [02:10<04:40, 14.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 10/30 [02:10<04:40, 14.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 10/30 [02:10<04:40, 14.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  33%|███▎      | 10/30 [02:10<04:40, 14.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 11/30 [02:23<04:21, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 11/30 [02:23<04:21, 13.75s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 11/30 [02:23<04:21, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 11/30 [02:23<04:21, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 11/30 [02:23<04:21, 13.76s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 11/30 [02:23<04:21, 13.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 11/30 [02:23<04:21, 13.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  37%|███▋      | 11/30 [02:23<04:21, 13.77s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  40%|████      | 12/30 [02:35<03:56, 13.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  40%|████      | 12/30 [02:35<03:56, 13.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  40%|████      | 12/30 [02:35<03:56, 13.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  40%|████      | 12/30 [02:35<03:56, 13.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  40%|████      | 12/30 [02:35<03:56, 13.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  40%|████      | 12/30 [02:35<03:56, 13.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  40%|████      | 12/30 [02:35<03:56, 13.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  40%|████      | 12/30 [02:35<03:56, 13.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  43%|████▎     | 13/30 [02:47<03:39, 12.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  43%|████▎     | 13/30 [02:47<03:39, 12.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  43%|████▎     | 13/30 [02:47<03:39, 12.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  43%|████▎     | 13/30 [02:47<03:39, 12.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  43%|████▎     | 13/30 [02:47<03:39, 12.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  43%|████▎     | 13/30 [02:47<03:39, 12.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  43%|████▎     | 13/30 [02:47<03:39, 12.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  43%|████▎     | 13/30 [02:47<03:39, 12.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 14/30 [02:57<03:12, 12.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 14/30 [02:57<03:12, 12.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 14/30 [02:57<03:12, 12.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 14/30 [02:57<03:12, 12.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 14/30 [02:57<03:12, 12.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 14/30 [02:57<03:12, 12.00s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 14/30 [02:57<03:12, 12.01s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  47%|████▋     | 14/30 [02:57<03:12, 12.02s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 15/30 [03:06<02:47, 11.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 15/30 [03:06<02:46, 11.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 15/30 [03:06<02:46, 11.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 15/30 [03:06<02:47, 11.13s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 15/30 [03:06<02:47, 11.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 15/30 [03:06<02:47, 11.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 15/30 [03:06<02:47, 11.14s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  50%|█████     | 15/30 [03:06<02:47, 11.15s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 16/30 [03:20<02:46, 11.87s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 16/30 [03:20<02:46, 11.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 16/30 [03:20<02:46, 11.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 16/30 [03:20<02:46, 11.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 16/30 [03:20<02:46, 11.88s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 16/30 [03:20<02:46, 11.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 16/30 [03:20<02:46, 11.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  53%|█████▎    | 16/30 [03:20<02:46, 11.89s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  57%|█████▋    | 17/30 [03:33<02:42, 12.48s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  57%|█████▋    | 17/30 [03:33<02:42, 12.48s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  57%|█████▋    | 17/30 [03:33<02:42, 12.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  57%|█████▋    | 17/30 [03:33<02:42, 12.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  57%|█████▋    | 17/30 [03:33<02:42, 12.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  57%|█████▋    | 17/30 [03:34<02:42, 12.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  57%|█████▋    | 17/30 [03:33<02:42, 12.50s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  57%|█████▋    | 17/30 [03:34<02:42, 12.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  60%|██████    | 18/30 [03:46<02:29, 12.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  60%|██████    | 18/30 [03:46<02:29, 12.44s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  60%|██████    | 18/30 [03:46<02:29, 12.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  60%|██████    | 18/30 [03:46<02:29, 12.45s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  60%|██████    | 18/30 [03:46<02:29, 12.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  60%|██████    | 18/30 [03:46<02:29, 12.47s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  60%|██████    | 18/30 [03:46<02:29, 12.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  60%|██████    | 18/30 [03:46<02:30, 12.50s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 19/30 [03:58<02:16, 12.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 19/30 [03:58<02:16, 12.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 19/30 [03:58<02:16, 12.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 19/30 [03:58<02:16, 12.39s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 19/30 [03:58<02:16, 12.39s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 19/30 [03:58<02:16, 12.39s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 19/30 [03:58<02:16, 12.40s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  63%|██████▎   | 19/30 [03:58<02:16, 12.41s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 20/30 [04:13<02:11, 13.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 20/30 [04:13<02:11, 13.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 20/30 [04:13<02:11, 13.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 20/30 [04:13<02:11, 13.10s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 20/30 [04:13<02:11, 13.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 20/30 [04:13<02:11, 13.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 20/30 [04:13<02:11, 13.12s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  67%|██████▋   | 20/30 [04:13<02:11, 13.11s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  70%|███████   | 21/30 [04:28<02:02, 13.57s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  70%|███████   | 21/30 [04:28<02:02, 13.56s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  70%|███████   | 21/30 [04:28<02:02, 13.57s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  70%|███████   | 21/30 [04:28<02:02, 13.57s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  70%|███████   | 21/30 [04:28<02:02, 13.57s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  70%|███████   | 21/30 [04:28<02:02, 13.58s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  70%|███████   | 21/30 [04:28<02:02, 13.59s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  70%|███████   | 21/30 [04:28<02:02, 13.59s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  73%|███████▎  | 22/30 [04:42<01:50, 13.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  73%|███████▎  | 22/30 [04:42<01:50, 13.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  73%|███████▎  | 22/30 [04:42<01:50, 13.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  73%|███████▎  | 22/30 [04:42<01:50, 13.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  73%|███████▎  | 22/30 [04:42<01:50, 13.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  73%|███████▎  | 22/30 [04:42<01:50, 13.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  73%|███████▎  | 22/30 [04:42<01:50, 13.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  73%|███████▎  | 22/30 [04:42<01:50, 13.83s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  77%|███████▋  | 23/30 [05:03<01:51, 15.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  77%|███████▋  | 23/30 [05:03<01:51, 15.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  77%|███████▋  | 23/30 [05:03<01:51, 15.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  77%|███████▋  | 23/30 [05:03<01:51, 15.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  77%|███████▋  | 23/30 [05:03<01:51, 15.90s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  77%|███████▋  | 23/30 [05:03<01:51, 15.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  77%|███████▋  | 23/30 [05:03<01:51, 15.91s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  77%|███████▋  | 23/30 [05:03<01:51, 15.92s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  80%|████████  | 24/30 [05:17<01:32, 15.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  80%|████████  | 24/30 [05:17<01:32, 15.50s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  80%|████████  | 24/30 [05:17<01:32, 15.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  80%|████████  | 24/30 [05:17<01:32, 15.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  80%|████████  | 24/30 [05:17<01:32, 15.49s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  80%|████████  | 24/30 [05:17<01:32, 15.50s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  80%|████████  | 24/30 [05:17<01:33, 15.50s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  80%|████████  | 24/30 [05:17<01:33, 15.51s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  83%|████████▎ | 25/30 [05:32<01:16, 15.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  83%|████████▎ | 25/30 [05:32<01:16, 15.31s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  83%|████████▎ | 25/30 [05:32<01:16, 15.30s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  83%|████████▎ | 25/30 [05:32<01:16, 15.31s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  83%|████████▎ | 25/30 [05:32<01:16, 15.32s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  83%|████████▎ | 25/30 [05:32<01:16, 15.31s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  83%|████████▎ | 25/30 [05:32<01:16, 15.31s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  83%|████████▎ | 25/30 [05:32<01:16, 15.31s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  87%|████████▋ | 26/30 [05:45<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  87%|████████▋ | 26/30 [05:45<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  87%|████████▋ | 26/30 [05:45<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  87%|████████▋ | 26/30 [05:45<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  87%|████████▋ | 26/30 [05:45<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  87%|████████▋ | 26/30 [05:45<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  87%|████████▋ | 26/30 [05:45<00:57, 14.46s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  87%|████████▋ | 26/30 [05:45<00:57, 14.47s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  90%|█████████ | 27/30 [06:04<00:47, 15.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  90%|█████████ | 27/30 [06:04<00:47, 15.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  90%|█████████ | 27/30 [06:04<00:47, 15.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  90%|█████████ | 27/30 [06:04<00:47, 15.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  90%|█████████ | 27/30 [06:04<00:47, 15.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  90%|█████████ | 27/30 [06:04<00:47, 15.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  90%|█████████ | 27/30 [06:04<00:47, 15.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  90%|█████████ | 27/30 [06:04<00:47, 15.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  93%|█████████▎| 28/30 [06:13<00:27, 13.97s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  93%|█████████▎| 28/30 [06:13<00:27, 13.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  93%|█████████▎| 28/30 [06:13<00:27, 13.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  93%|█████████▎| 28/30 [06:13<00:27, 13.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  93%|█████████▎| 28/30 [06:13<00:27, 13.97s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  93%|█████████▎| 28/30 [06:13<00:27, 13.98s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  93%|█████████▎| 28/30 [06:13<00:27, 13.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  93%|█████████▎| 28/30 [06:13<00:27, 13.99s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  97%|█████████▋| 29/30 [06:30<00:14, 14.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  97%|█████████▋| 29/30 [06:30<00:14, 14.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  97%|█████████▋| 29/30 [06:30<00:14, 14.81s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  97%|█████████▋| 29/30 [06:30<00:14, 14.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  97%|█████████▋| 29/30 [06:30<00:14, 14.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  97%|█████████▋| 29/30 [06:30<00:14, 14.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  97%|█████████▋| 29/30 [06:30<00:14, 14.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards:  97%|█████████▋| 29/30 [06:30<00:14, 14.82s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 13.16s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 11.68s/it]\u001b[0m\n",
      "\u001b[34mDownloading shards: 100%|██████████| 30/30 [06:34<00:00, 13.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:01<00:48,  1.68s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:02<01:14,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:02<01:15,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:02<01:16,  2.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:02<01:15,  2.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:02<01:14,  2.58s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:02<01:15,  2.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:02<01:16,  2.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:03<00:53,  1.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:04<01:08,  2.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:04<01:09,  2.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:05<01:11,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:05<01:10,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:05<01:10,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:05<01:10,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:05<01:11,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:05<00:54,  2.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:07<01:06,  2.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:07<01:07,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:07<01:07,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:07<01:07,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:07<01:07,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:07<01:07,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:07<01:09,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:08<00:54,  2.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:09<01:04,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:10<01:05,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:10<01:05,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:10<00:51,  2.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:10<01:05,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:10<01:05,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:10<01:05,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:10<01:06,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:12<00:49,  2.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:00,  2.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:01,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:01,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:01,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:01,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:01,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:12<01:02,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:14<00:47,  2.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:14<00:57,  2.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:14<00:58,  2.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:14<00:59,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:14<00:59,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:14<00:59,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:14<00:59,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:15<00:59,  2.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:16<00:46,  2.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:16<00:54,  2.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:17<00:55,  2.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:17<00:56,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:17<00:56,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:17<00:56,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:17<00:56,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:17<00:56,  2.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:18<00:44,  2.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:19<00:53,  2.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:19<00:54,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:20<00:54,  2.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:20<00:55,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:20<00:55,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:19<00:55,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:20<00:55,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:20<00:41,  2.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:21<00:50,  2.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:22<00:52,  2.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:22<00:52,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:22<00:53,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:22<00:53,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:22<00:53,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:22<00:53,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:22<00:39,  2.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:24<00:47,  2.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:24<00:49,  2.45s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [00:24<00:37,  2.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:25<00:49,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:25<00:50,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:25<00:50,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:25<00:50,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:25<00:50,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:26<00:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [00:26<00:35,  2.10s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:27<00:46,  2.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:27<00:47,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:27<00:47,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:27<00:47,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:27<00:47,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:27<00:47,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [00:28<00:42,  2.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [00:29<00:33,  2.11s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [00:29<00:43,  2.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [00:30<00:44,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [00:30<00:44,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [00:30<00:44,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [00:30<00:44,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [00:30<00:45,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [00:31<00:31,  2.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [00:31<00:40,  2.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [00:32<00:41,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [00:32<00:43,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [00:32<00:43,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [00:32<00:43,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [00:32<00:43,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [00:32<00:43,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [00:33<00:28,  2.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [00:33<00:38,  2.41s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [00:34<00:39,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [00:35<00:26,  2.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [00:35<00:40,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [00:35<00:40,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [00:35<00:40,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [00:35<00:40,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [00:35<00:40,  2.56s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [00:36<00:35,  2.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [00:36<00:36,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [00:37<00:25,  2.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [00:37<00:38,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [00:37<00:38,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [00:37<00:38,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [00:37<00:38,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [00:37<00:38,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [00:38<00:33,  2.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [00:39<00:34,  2.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [00:39<00:23,  2.12s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [00:40<00:35,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [00:40<00:35,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [00:40<00:35,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [00:40<00:35,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [00:40<00:35,  2.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [00:40<00:30,  2.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [00:41<00:20,  2.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [00:41<00:31,  2.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [00:42<00:32,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [00:42<00:32,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [00:42<00:32,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [00:42<00:32,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [00:42<00:32,  2.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [00:43<00:28,  2.39s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [00:43<00:18,  2.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [00:44<00:29,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [00:45<00:30,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [00:45<00:30,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [00:45<00:30,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [00:45<00:30,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [00:45<00:30,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [00:45<00:16,  2.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [00:45<00:26,  2.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [00:46<00:27,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [00:47<00:14,  2.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [00:47<00:23,  2.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [00:48<00:28,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [00:47<00:28,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [00:47<00:28,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [00:47<00:28,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [00:48<00:28,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [00:49<00:24,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [00:49<00:12,  2.11s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [00:50<00:21,  2.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [00:50<00:25,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [00:50<00:25,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [00:50<00:25,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [00:50<00:25,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [00:50<00:25,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [00:51<00:22,  2.46s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [00:51<00:10,  2.08s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [00:52<00:18,  2.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [00:53<00:22,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [00:53<00:22,  2.54s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [00:52<00:22,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [00:52<00:22,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [00:53<00:22,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  87%|████████▋ | 26/30 [00:53<00:08,  2.07s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [00:54<00:19,  2.44s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [00:55<00:16,  2.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [00:55<00:20,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [00:55<00:20,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [00:55<00:20,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [00:55<00:20,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [00:55<00:20,  2.53s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [00:56<00:06,  2.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [00:56<00:17,  2.48s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [00:57<00:14,  2.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [00:58<00:04,  2.09s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [00:58<00:17,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [00:58<00:18,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [00:58<00:17,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [00:58<00:17,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [00:58<00:17,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [00:59<00:15,  2.50s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [00:59<00:11,  2.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [01:00<00:02,  2.11s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [01:00<00:15,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [01:00<00:15,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [01:00<00:15,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [01:00<00:15,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [01:00<00:15,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:00<00:00,  1.66s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:00<00:00,  2.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [01:01<00:12,  2.47s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  87%|████████▋ | 26/30 [01:02<00:09,  2.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [01:03<00:12,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [01:03<00:12,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [01:03<00:12,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [01:03<00:12,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [01:03<00:12,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  87%|████████▋ | 26/30 [01:04<00:09,  2.45s/it]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [01:04<00:07,  2.37s/it]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1 examples [00:00,  2.06 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 793 examples [00:00, 1282.64 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 1590 examples [00:01, 1754.95 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 2389 examples [00:01, 2025.97 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3181 examples [00:01, 2211.86 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  87%|████████▋ | 26/30 [01:05<00:10,  2.59s/it]#015Loading checkpoint shards:  87%|████████▋ | 26/30 [01:05<00:10,  2.59s/it]#015Loading checkpoint shards:  87%|████████▋ | 26/30 [01:05<00:10,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  87%|████████▋ | 26/30 [01:05<00:10,  2.59s/it]\u001b[0m\n",
      "\u001b[34mGenerating train split: 3976 examples [00:02, 2377.96 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  87%|████████▋ | 26/30 [01:06<00:10,  2.67s/it]\u001b[0m\n",
      "\u001b[34mGenerating train split: 4737 examples [00:02, 3076.09 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5568 examples [00:02, 2558.99 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 5904 examples [00:02, 2256.20 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 315 examples [00:00, 2922.66 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 315 examples [00:00, 2561.31 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [01:07<00:07,  2.65s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [01:07<00:05,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [01:08<00:07,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [01:08<00:07,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [01:08<00:07,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [01:08<00:07,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [01:08<00:07,  2.64s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [01:09<00:05,  2.62s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [01:10<00:02,  2.55s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:10<00:00,  1.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:10<00:00,  2.36s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [01:11<00:05,  2.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [01:11<00:05,  2.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [01:11<00:05,  2.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [01:11<00:05,  2.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [01:11<00:05,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [01:12<00:02,  2.59s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:13<00:00,  2.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:13<00:00,  2.43s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [01:13<00:02,  2.57s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [01:13<00:02,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [01:13<00:02,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [01:13<00:02,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [01:13<00:02,  2.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.06s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [01:14<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34mtrainable params: 207,093,760 || all params: 70,760,800,256 || trainable%: 0.2926673514866587\u001b[0m\n",
      "\u001b[34m0%|          | 0/92 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 1/92 [00:40<1:02:09, 40.98s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/92 [01:04<46:06, 30.74s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/92 [01:28<40:42, 27.44s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 4/92 [01:51<37:58, 25.90s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 5/92 [02:15<36:25, 25.12s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 6/92 [02:39<35:22, 24.68s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 7/92 [03:02<34:25, 24.30s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 8/92 [03:26<33:39, 24.04s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 9/92 [03:49<33:01, 23.87s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 10/92 [04:13<32:41, 23.92s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.8873, 'grad_norm': 0.09814453125, 'learning_rate': 0.0002, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m11%|█         | 10/92 [04:13<32:41, 23.92s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 11/92 [04:37<32:07, 23.80s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 12/92 [05:00<31:34, 23.69s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 13/92 [05:24<31:04, 23.61s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 14/92 [05:47<30:47, 23.69s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 15/92 [06:11<30:17, 23.61s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 16/92 [06:34<29:50, 23.55s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 17/92 [06:58<29:24, 23.53s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 18/92 [07:21<29:00, 23.52s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 19/92 [07:45<28:47, 23.66s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 20/92 [08:09<28:21, 23.63s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.656, 'grad_norm': 0.0673828125, 'learning_rate': 0.0002, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 20/92 [08:09<28:21, 23.63s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 21/92 [08:32<27:57, 23.62s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 22/92 [08:56<27:31, 23.59s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 23/92 [09:20<27:14, 23.69s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 24/92 [09:43<26:46, 23.62s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 25/92 [10:07<26:19, 23.58s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 26/92 [10:30<25:54, 23.56s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 27/92 [10:54<25:36, 23.63s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 28/92 [11:18<25:14, 23.67s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 29/92 [11:41<24:48, 23.62s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 30/92 [12:05<24:23, 23.61s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6351, 'grad_norm': 0.048583984375, 'learning_rate': 0.0002, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 30/92 [12:05<24:23, 23.61s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 31/92 [12:29<23:59, 23.59s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 32/92 [12:53<23:43, 23.72s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 33/92 [13:16<23:17, 23.68s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 34/92 [13:40<22:50, 23.64s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 35/92 [14:03<22:25, 23.60s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 36/92 [14:27<22:09, 23.74s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 37/92 [14:51<21:43, 23.70s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 38/92 [15:14<21:17, 23.66s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 39/92 [15:38<20:51, 23.62s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 40/92 [16:02<20:30, 23.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6235, 'grad_norm': 0.041748046875, 'learning_rate': 0.0002, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 40/92 [16:02<20:30, 23.67s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 41/92 [16:25<20:08, 23.69s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 43/92 [17:13<19:17, 23.62s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 44/92 [17:36<18:53, 23.61s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 45/92 [18:00<18:35, 23.74s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 46/92 [18:24<18:09, 23.68s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:01<00:36,  1.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:03<00:50,  1.36s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:05<00:56,  1.57s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:07<00:59,  1.69s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:09<01:00,  1.77s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:11<00:59,  1.82s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:13<00:59,  1.85s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:15<00:57,  1.87s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:17<00:56,  1.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:19<00:54,  1.89s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:21<00:53,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:23<00:51,  1.90s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:24<00:49,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:26<00:47,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:28<00:45,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:30<00:44,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:32<00:42,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:34<00:40,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:36<00:38,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:38<00:36,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:40<00:34,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:42<00:32,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:44<00:30,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:46<00:28,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:47<00:26,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:49<00:24,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:51<00:22,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 29/40 [00:53<00:21,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:55<00:19,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:57<00:17,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [00:59<00:15,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [01:01<00:13,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [01:03<00:11,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [01:05<00:09,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [01:07<00:07,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [01:09<00:05,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [01:10<00:03,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [01:12<00:01,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [01:14<00:00,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.6276493072509766, 'eval_runtime': 76.6603, 'eval_samples_per_second': 4.109, 'eval_steps_per_second': 0.522, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [01:14<00:00,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 46/92 [19:52<18:09, 23.68s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 47/92 [21:53<59:33, 79.41s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 48/92 [22:17<46:03, 62.81s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 49/92 [22:41<36:34, 51.04s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 50/92 [23:04<29:58, 42.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6068, 'grad_norm': 0.0546875, 'learning_rate': 0.0002, 'epoch': 1.08}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 50/92 [23:05<29:58, 42.81s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 51/92 [23:28<25:19, 37.07s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 52/92 [23:52<22:01, 33.03s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 53/92 [24:16<19:44, 30.37s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 54/92 [24:39<17:56, 28.34s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 55/92 [25:03<16:35, 26.91s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 56/92 [25:27<15:33, 25.92s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 57/92 [25:51<14:48, 25.38s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 58/92 [26:14<14:05, 24.86s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 59/92 [26:38<13:27, 24.48s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 60/92 [27:02<12:55, 24.22s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6064, 'grad_norm': 0.064453125, 'learning_rate': 0.0002, 'epoch': 1.29}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 60/92 [27:02<12:55, 24.22s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 61/92 [27:25<12:25, 24.05s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 62/92 [27:49<12:01, 24.06s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 63/92 [28:13<11:33, 23.92s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 64/92 [28:37<11:07, 23.83s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 65/92 [29:00<10:41, 23.77s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 66/92 [29:24<10:20, 23.86s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 67/92 [29:48<09:54, 23.79s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 68/92 [30:11<09:29, 23.73s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 69/92 [30:35<09:05, 23.71s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 70/92 [30:59<08:44, 23.83s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5806, 'grad_norm': 0.04443359375, 'learning_rate': 0.0002, 'epoch': 1.51}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 70/92 [30:59<08:44, 23.83s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 71/92 [31:23<08:19, 23.78s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 72/92 [31:47<07:54, 23.73s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 73/92 [32:10<07:30, 23.69s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 74/92 [32:34<07:05, 23.66s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 75/92 [32:58<06:44, 23.79s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 76/92 [33:21<06:19, 23.74s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 77/92 [33:45<05:55, 23.70s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 78/92 [34:09<05:31, 23.67s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 79/92 [34:33<05:09, 23.81s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 80/92 [34:56<04:44, 23.74s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.6054, 'grad_norm': 0.0380859375, 'learning_rate': 0.0002, 'epoch': 1.72}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 80/92 [34:56<04:44, 23.74s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 81/92 [35:20<04:20, 23.71s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 82/92 [35:44<03:56, 23.68s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 83/92 [36:07<03:33, 23.73s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 84/92 [36:31<03:10, 23.76s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 85/92 [36:55<02:45, 23.70s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 86/92 [37:18<02:22, 23.67s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 87/92 [37:42<01:58, 23.65s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 88/92 [38:06<01:35, 23.77s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 89/92 [38:30<01:11, 23.71s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 90/92 [38:53<00:47, 23.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.5819, 'grad_norm': 0.0400390625, 'learning_rate': 0.0002, 'epoch': 1.94}\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 90/92 [38:53<00:47, 23.67s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 91/92 [39:17<00:23, 23.67s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 92/92 [39:41<00:00, 23.72s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 0/40 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 2/40 [00:01<00:37,  1.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 3/40 [00:03<00:51,  1.40s/it]#033[A\u001b[0m\n",
      "\u001b[34m10%|█         | 4/40 [00:05<00:58,  1.62s/it]#033[A\u001b[0m\n",
      "\u001b[34m12%|█▎        | 5/40 [00:07<01:01,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[34m15%|█▌        | 6/40 [00:09<01:02,  1.83s/it]#033[A\u001b[0m\n",
      "\u001b[34m18%|█▊        | 7/40 [00:11<01:01,  1.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 8/40 [00:13<01:01,  1.91s/it]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▎       | 9/40 [00:15<01:00,  1.94s/it]#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 10/40 [00:17<00:58,  1.95s/it]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 11/40 [00:19<00:56,  1.96s/it]#033[A\u001b[0m\n",
      "\u001b[34m30%|███       | 12/40 [00:21<00:55,  1.97s/it]#033[A\u001b[0m\n",
      "\u001b[34m32%|███▎      | 13/40 [00:23<00:53,  1.97s/it]#033[A\u001b[0m\n",
      "\u001b[34m35%|███▌      | 14/40 [00:25<00:51,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m38%|███▊      | 15/40 [00:27<00:49,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 16/40 [00:29<00:47,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m42%|████▎     | 17/40 [00:31<00:45,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 18/40 [00:33<00:43,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 19/40 [00:35<00:41,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 20/40 [00:37<00:39,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▎    | 21/40 [00:39<00:37,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 22/40 [00:41<00:35,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▊    | 23/40 [00:43<00:33,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 24/40 [00:45<00:31,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m62%|██████▎   | 25/40 [00:47<00:29,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 26/40 [00:49<00:27,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 27/40 [00:51<00:25,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m70%|███████   | 28/40 [00:53<00:23,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▎  | 29/40 [00:55<00:21,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 30/40 [00:57<00:19,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 31/40 [00:59<00:17,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 32/40 [01:01<00:15,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m82%|████████▎ | 33/40 [01:03<00:13,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 34/40 [01:05<00:11,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 35/40 [01:07<00:09,  1.98s/it]#033[A\u001b[0m\n",
      "\u001b[34m90%|█████████ | 36/40 [01:09<00:07,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▎| 37/40 [01:11<00:05,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 38/40 [01:13<00:03,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 39/40 [01:15<00:01,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [01:17<00:00,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 1.6159824132919312, 'eval_runtime': 79.3764, 'eval_samples_per_second': 3.968, 'eval_steps_per_second': 0.504, 'epoch': 1.98}\u001b[0m\n",
      "\u001b[34m100%|██████████| 40/40 [01:17<00:00,  1.99s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 92/92 [41:00<00:00, 23.72s/it]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'train_runtime': 2568.3248, 'train_samples_per_second': 4.598, 'train_steps_per_second': 0.036, 'train_loss': 1.640997772631438, 'epoch': 1.98}\u001b[0m\n",
      "\u001b[34m100%|██████████| 92/92 [42:48<00:00, 23.72s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 92/92 [42:48<00:00, 27.92s/it]\u001b[0m\n",
      "\u001b[34mTrying to load a Peft model. It might take a while without feedback\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[W PyInterpreter.cpp:221] Warning: Deallocating Tensor that still has live PyObject references.  This probably happened because you took out a weak reference to Tensor and didn't call _fix_weakref() after dereferencing it.  Subsequent accesses to this tensor via the PyObject will now fail. (function decref)\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   3%|▎         | 1/30 [00:05<02:52,  5.96s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   7%|▋         | 2/30 [00:10<02:28,  5.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  10%|█         | 3/30 [00:15<02:20,  5.21s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  13%|█▎        | 4/30 [00:20<02:14,  5.16s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  17%|█▋        | 5/30 [00:25<02:05,  5.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  20%|██        | 6/30 [00:30<01:58,  4.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  23%|██▎       | 7/30 [00:35<01:52,  4.88s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  27%|██▋       | 8/30 [00:40<01:49,  4.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  30%|███       | 9/30 [00:45<01:45,  5.02s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  33%|███▎      | 10/30 [00:50<01:39,  4.96s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  37%|███▋      | 11/30 [00:55<01:33,  4.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  40%|████      | 12/30 [01:00<01:28,  4.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  43%|████▎     | 13/30 [01:05<01:24,  4.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  47%|████▋     | 14/30 [01:10<01:20,  5.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 15/30 [01:15<01:14,  4.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  53%|█████▎    | 16/30 [01:20<01:09,  4.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  57%|█████▋    | 17/30 [01:24<01:03,  4.90s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  60%|██████    | 18/30 [01:30<00:59,  4.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  63%|██████▎   | 19/30 [01:35<00:55,  5.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  67%|██████▋   | 20/30 [01:40<00:49,  4.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  70%|███████   | 21/30 [01:44<00:44,  4.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  73%|███████▎  | 22/30 [01:49<00:39,  4.91s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  77%|███████▋  | 23/30 [01:54<00:34,  4.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  80%|████████  | 24/30 [02:00<00:30,  5.03s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  83%|████████▎ | 25/30 [02:04<00:24,  4.97s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  87%|████████▋ | 26/30 [02:09<00:19,  4.93s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  90%|█████████ | 27/30 [02:14<00:14,  4.91s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  93%|█████████▎| 28/30 [02:19<00:09,  4.99s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  97%|█████████▋| 29/30 [02:24<00:05,  5.04s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [02:27<00:00,  4.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 30/30 [02:27<00:00,  4.91s/it]\u001b[0m\n",
      "\u001b[34mSaving the newly created merged model to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2024-06-25 13:59:16,117 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-06-25 13:59:16,117 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-06-25 13:59:16,117 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-06-25 14:00:18 Uploading - Uploading generated training model\n",
      "2024-06-25 14:08:01 Completed - Training job completed\n",
      "Training seconds: 4975\n",
      "Billable seconds: 4975\n"
     ]
    }
   ],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "  'train': train_dataset_s3_path,\n",
    "  'test': test_dataset_s3_path,\n",
    "  'config': train_config_s3_path\n",
    "  }\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example the training Llama 3 70B with Flash Attention for 2 epochs with a dataset of 10k samples takes 5052 seconds (~84minutes) on a `ml.p4d.24xlarge` or ~$50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Deploy & Test fine-tuned Llama 3 on Amazon SageMaker\n",
    "\n",
    "Evaluating LLMs is crucial for understanding their capabilities and limitations, yet it poses significant challenges due to their complex and opaque nature. There are multiple ways to evaluate a fine-tuned model. You could either use an additional Training job to evaluate the model as we demonstrated in [Evaluate LLMs with Hugging Face Lighteval on Amazon SageMaker](https://www.philschmid.de/sagemaker-evaluate-llm-lighteval) or you can deploy the model to an endpoint and interactively test the model. We are going to use the latter approach in this example. We will load our eval dataset and evaluate the model on those samples, using a simple loop and accuracy as our metric.\n",
    "\n",
    "_Note: Evaluating Generative AI models is not a trivial task since 1 input can have multiple correct outputs. If you want to learn more about evaluating generative models, check out [Evaluate LLMs and RAG a practical example using Langchain and Hugging Face](https://www.philschmid.de/evaluate-llm) blog post._\n",
    "\n",
    "We are going to use the [Hugging Face LLM Inference DLC](https://huggingface.co/blog/sagemaker-huggingface-llm#what-is-hugging-face-llm-inference-dlc) a purpose-built Inference Container to easily deploy LLMs in a secure and managed environment. The DLC is powered by [Text Generation Inference (TGI)](https://huggingface.co/docs/text-generation-inference/index) solution for deploying and serving Large Language Models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py310\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: gpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.2-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "  \"huggingface\",\n",
    "  session=sess,\n",
    "  version=\"2.0.2\",\n",
    ")\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a `HuggingFaceModel` using the container uri and the S3 path to our model. We also need to set our TGI configuration including the number of GPUs, max input tokens. You can find a full list of configuration options [here](https://huggingface.co/docs/text-generation-inference/basic_tutorials/launcher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "health_check_timeout = 1200 # 20 minutes\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"/opt/ml/model\",       # Path to the model in the container\n",
    "  'SM_NUM_GPUS': \"8\",                   # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"8000\",           # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"8096\",           # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_PREFILL_TOKENS': \"16182\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'MESSAGES_API_ENABLED': \"true\",       # Enable the OpenAI Messages API\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  # path to s3 bucket with model, we are not using a compressed model\n",
    "  # {'S3DataSource':{'S3Uri': \"s3://...\",'S3DataType': 'S3Prefix','CompressionType': 'None'}},\n",
    "  model_data=huggingface_estimator.model_data,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the HuggingFaceModel we can deploy it to Amazon SageMaker using the deploy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: huggingface-pytorch-tgi-inference-2024-06-25-14-08-38-968\n",
      "INFO:sagemaker:Creating endpoint-config with name huggingface-pytorch-tgi-inference-2024-06-25-14-08-39-615\n",
      "INFO:sagemaker:Creating endpoint with name huggingface-pytorch-tgi-inference-2024-06-25-14-08-39-615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 20 minutes to give SageMaker the time to download and merge model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 15-20 minutes. Afterwards, we can test our model by sending some example inputs to the endpoint. We will use the `predict` method of the predictor to send the input to the model and get the output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages=[\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"Tell me something about Amazon SageMaker?\" }\n",
    "  ]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": \"meta-llama-3-fine-tuned\", # placeholder, needed\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high-quality models.\n"
     ]
    }
   ],
   "source": [
    "chat = llm.predict({\"messages\" :messages, **parameters})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: If you want to learn more about streaming responses or benchmarking your endpoint checkout [Deploy Llama 3 on Amazon SageMaker](https://www.philschmid.de/sagemaker-llama3) and [Deploy Mixtral 8x7B on Amazon SageMaker\n",
    "](https://www.philschmid.de/sagemaker-deploy-mixtral)._\n",
    "\n",
    "To clean up, we can delete the model and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: huggingface-pytorch-tgi-inference-2024-06-25-14-08-38-968\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: huggingface-pytorch-tgi-inference-2024-06-25-14-08-39-615\n",
      "INFO:sagemaker:Deleting endpoint with name: huggingface-pytorch-tgi-inference-2024-06-25-14-08-39-615\n"
     ]
    }
   ],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
