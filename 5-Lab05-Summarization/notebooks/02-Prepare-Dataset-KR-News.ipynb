{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# 데이터셋 준비하기\n",
    "- https://www.kaggle.com/code/mitanshuchakrawarty/fine-tune-llm-for-text-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b5f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "!huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdcebd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/home/ec2-user/SageMaker/.cache\" \n",
    "os.environ['HF_DATASETS_CACHE'] = \"/home/ec2-user/SageMaker/.cache\" \n",
    "os.environ['HF_HOME'] = \"/home/ec2-user/SageMaker/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b722-f752-4687-8fbe-12855566892c",
   "metadata": {},
   "source": [
    "## 2. 데이터 셋 준비\n",
    "### 데이터 셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3710c3d-71bb-47b1-97eb-a72ef6dcfd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 22194\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2466\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"daekeun-ml/naver-news-summarization-ko\"\n",
    "\n",
    "# dataset = load_dataset(huggingface_dataset_name, \"3.0.0\")\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94062514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96451467-4fc0-49af-bedf-040850c11fbb",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eda452",
   "metadata": {},
   "source": [
    "### Chat Message 형태 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1181ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_instruction(system_prompt: str, article: str, summary: str):\n",
    "    message = [\n",
    "            {\n",
    "                'content': system_prompt,\n",
    "                'role': 'system'\n",
    "            },\n",
    "            {\n",
    "                'content': f'Please summarize the goals for journalist in this text:\\n\\n{article}',\n",
    "                'role': 'user'\n",
    "            },\n",
    "            {\n",
    "                'content': f'{summary}',\n",
    "                'role': 'assistant'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    return message # json.dumps(message, indent=2) # json.dumps(message, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "# system_prompt = \"You are an AI assistant specialized in news articles. Your role is to provide accurate summaries and insights. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\"\n",
    "# article = \"Within three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert's portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. \\\"We can't ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,\\\" says Rinkert.\"\n",
    "# summary = \"Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.\"\n",
    "\n",
    "# print(format_instruction(system_prompt, article, summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a4269",
   "metadata": {},
   "source": [
    "### Chat Message 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fede492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'category', 'press', 'title', 'document', 'link', 'summary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5a43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "    system_prompt = \"You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": format_instruction(system_prompt, data_point[\"document\"],data_point[\"summary\"])\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_instruction_dataset).remove_columns(columns_to_remove)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390bfdf",
   "metadata": {},
   "source": [
    "#### 전체 데이터 셋에서 일부 데티터 추출 (짧은 실습을 위해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75ddc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_debug = 10\n",
    "validation_num_debug = 10\n",
    "test_num_debug = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0edb7874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 10\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def create_message_dataset(dataset, num_train,num_val, num_test, verbose=False):\n",
    "    ## APPLYING PREPROCESSING ON WHOLE DATASET\n",
    "\n",
    "    train_dataset = process_dataset(dataset[\"train\"].select(range(num_train)))\n",
    "    validation_dataset = process_dataset(dataset[\"validation\"].select(range(num_val)))\n",
    "    test_dataset= process_dataset(dataset[\"test\"].select(range(num_test)))\n",
    "    \n",
    "    if verbose:\n",
    "        print(train_dataset)\n",
    "        print(test_dataset)\n",
    "        print(validation_dataset)\n",
    "\n",
    "    return train_dataset,test_dataset,validation_dataset    \n",
    "\n",
    "train_dataset,test_dataset,validation_dataset = create_message_dataset(dataset=dataset, \n",
    "                                                num_train=train_num_debug,\n",
    "                                                num_val=validation_num_debug, \n",
    "                                                num_test=test_num_debug, verbose=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7197aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b678c9",
   "metadata": {},
   "source": [
    "#### 전체 데이터 셋 저장 (성능 측정을 위해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dec1d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num_samples:  1000\n",
      "validation_num_samples:  1000\n",
      "test_num_samples:  1000\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# full_train_num = len(dataset[\"train\"])\n",
    "# full_validation_num = len(dataset[\"validation\"])\n",
    "# full_test_num = len(dataset[\"test\"])\n",
    "\n",
    "full_train_num = 1000\n",
    "full_validation_num = 1000\n",
    "full_test_num = 1000\n",
    "\n",
    "print(\"train_num_samples: \", full_train_num)\n",
    "print(\"validation_num_samples: \", full_validation_num)\n",
    "print(\"test_num_samples: \", full_test_num)\n",
    "\n",
    "full_train_dataset,full_test_dataset,full_validation_dataset = create_message_dataset(dataset=dataset, \n",
    "                                                                num_train=full_train_num,\n",
    "                                                                num_val=full_validation_num, \n",
    "                                                                num_test=full_test_num, verbose=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dda7e7",
   "metadata": {},
   "source": [
    "## 4. 데이터 셋을 JSON 으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8af3619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_dataset_json_file(huggingface_dataset_name,train_dataset, validation_dataset, test_dataset, is_full, verbose=True ):\n",
    "    dataset_name = huggingface_dataset_name.split(\"/\")[1]\n",
    "    data_folder = os.path.join(\"../data/\",dataset_name)\n",
    "    os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "    if is_full:\n",
    "        train_data_json = os.path.join(data_folder,\"full_train\", \"train_dataset.json\")\n",
    "        validation_data_json = os.path.join(data_folder,\"full_validation\", \"validation_dataset.json\")\n",
    "        test_data_json = os.path.join(data_folder, \"full_test\", \"test_dataset.json\")\n",
    "\n",
    "    else:\n",
    "        train_data_json = os.path.join(data_folder,\"train\", \"train_dataset.json\")\n",
    "        validation_data_json = os.path.join(data_folder,\"validation\", \"validation_dataset.json\")\n",
    "        test_data_json = os.path.join(data_folder, \"test\", \"test_dataset.json\")\n",
    "\n",
    "    # save datasets to disk \n",
    "    train_dataset.to_json(train_data_json, orient=\"records\", force_ascii=False)\n",
    "    validation_dataset.to_json(validation_data_json, orient=\"records\", force_ascii=False)\n",
    "    test_dataset.to_json(test_data_json, orient=\"records\", force_ascii=False)        \n",
    "\n",
    "    if verbose:\n",
    "        print(train_dataset)\n",
    "        print(f\"{train_data_json} is saved\")\n",
    "        print(f\"{validation_data_json} is saved\")\n",
    "        print(f\"{test_data_json} is saved\")                \n",
    "\n",
    "    return data_folder, train_data_json, validation_data_json, test_data_json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6c2cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 351.66ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 973.61ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 914.99ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 10\n",
      "})\n",
      "../data/naver-news-summarization-ko/train/train_dataset.json is saved\n",
      "../data/naver-news-summarization-ko/validation/validation_dataset.json is saved\n",
      "../data/naver-news-summarization-ko/test/test_dataset.json is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 16.25ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 17.82ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 17.43ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "../data/naver-news-summarization-ko/full_train/train_dataset.json is saved\n",
      "../data/naver-news-summarization-ko/full_validation/validation_dataset.json is saved\n",
      "../data/naver-news-summarization-ko/full_test/test_dataset.json is saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store debug dataset\n",
    "data_folder, train_data_json, validation_data_json, test_data_json = create_dataset_json_file(huggingface_dataset_name=huggingface_dataset_name,\n",
    "                                                                    train_dataset=train_dataset, \n",
    "                                                                    validation_dataset=validation_dataset, \n",
    "                                                                    test_dataset=test_dataset,\n",
    "                                                                    is_full=False )        \n",
    "\n",
    "# Store full dataset\n",
    "data_folder, full_train_data_json, full_validation_data_json, full_test_data_json = create_dataset_json_file(huggingface_dataset_name=huggingface_dataset_name,\n",
    "                                                                    train_dataset=full_train_dataset, \n",
    "                                                                    validation_dataset=full_validation_dataset, \n",
    "                                                                    test_dataset=full_test_dataset,\n",
    "                                                                    is_full=True )        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c521c6f",
   "metadata": {},
   "source": [
    "### 다음 노트북에서 사용하기 위해 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70fb035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data_folder' (str)\n",
      "Stored 'train_data_json' (str)\n",
      "Stored 'validation_data_json' (str)\n",
      "Stored 'test_data_json' (str)\n",
      "Stored 'full_train_data_json' (str)\n",
      "Stored 'full_validation_data_json' (str)\n",
      "Stored 'full_test_data_json' (str)\n"
     ]
    }
   ],
   "source": [
    "%store data_folder\n",
    "%store train_data_json \n",
    "%store validation_data_json \n",
    "%store test_data_json \n",
    "%store full_train_data_json \n",
    "%store full_validation_data_json \n",
    "%store full_test_data_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5c4cc",
   "metadata": {},
   "source": [
    "## 5. Option: 데이터 셋을 ChatTemplate 형태로 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165e725",
   "metadata": {},
   "source": [
    "### Chat Template 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4ba09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] }}{% elif message['role'] == 'user' %}{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}{% elif message['role'] == 'assistant' %}{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\\n\\nAssistant: ' }}{% endif %}\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfcc19",
   "metadata": {},
   "source": [
    "### Chat Template 으로 변형하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54451f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer        \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "\n",
    "# template dataset\n",
    "def template_dataset(examples):\n",
    "    return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "\n",
    "sample_train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "# sample_test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "# sample_validation_dataset = validation_dataset.map(template_dataset, remove_columns=[\"messages\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fc688",
   "metadata": {},
   "source": [
    "### 변형된 Chat Message 형태 예시 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  3\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "미국 뉴욕증시가 52년 만에 최악의 상반기를 보낸 가운데 씨티그룹이 랠리가 기대되는 종목들을 추천했다. 30일 현지시간 CNBC에 따르면 씨티은행은 팩트셋 자료를 인용해 올해 안으로 20% 이상의 주가 상승세를 보일 기업들을 꼽았다. 해당 기업들에 최소 10% 이상의 주당 순이익 성장세가 예상된다는 분석이다. 씨티그룹이 가장 먼저 꼽은 기업은 마이크로소프트다. 마이크로소프트는 올 들어 현재까지 주가가 23% 이상 하락했다. 그러나 분석가들은 주가가 현재 수준에서 36% 반등할 것으로 보고 있다. 이날 마이크로소프트는 전 거래일 대비 1.32% 하락한 256.83달러에서 거래를 마감했다. 씨티그룹은 마이크로소프트가 IT 사업 예산을 늘렸다는 점과 마이크로소프트가 추진하는 하이브리드 업무환경을 긍정적으로 평가하고 있다며 올해 주당 순이익이 16.5% 뛸 것으로 예상한다고 덧붙였다. 이외에도 씨티그룹은 엔터테인먼트와 에너지주에 주목했다. 디즈니의 주가는 올 들어 39% 급락하면서 52주 최고가 대비 반토막 났다. 씨티그룹은 디즈니의 주가가 이날 종가 94.40달러 대비 50% 반등할 수 있다며 주당 순이익도 올해 약 74%까지 오를 수 있다고 내다봤다. 씨티그룹은 에너지주의 랠리 가능성 또한 높게 점쳤다. 에너지주는 올 상반기 S P 500 지수 내 유일하게 올해 30% 이상 상승했다. 씨티그룹은 에너지주 중에서도 셰브론의 주당 순이익이 올해 내 두 배 이상 뛸 것이라며 유망주로 꼽았다.<|end_of_text|>\n",
      "\n",
      "Assistant: 30일 현지시간 CNBC에 따르면 씨티은행은 팩트셋 자료를 인용해 올해 안으로 20% 이상의 주가 상승세를 보일 기업들을 뽑았는데 마이크로소프트가  IT 사업 예산을 늘렸다는 점과 마이크로소프트가 추진하는 하이브리드 업무환경을 긍정적으로 평가하고 있다며 올해 주당 순이익이 16.5% 뛸 것으로 예상한다고 덧붙였다.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# print random sample\n",
    "import random\n",
    "\n",
    "for index in random.sample(range(len(sample_train_dataset)), 1):\n",
    "    print(\"index: \", index)\n",
    "    # index = 5343\n",
    "    print(sample_train_dataset[index][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60aaa4-b654-49b3-bb87-34ad6a9f375c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88bd97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a7201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68ddc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llama3_puy310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
