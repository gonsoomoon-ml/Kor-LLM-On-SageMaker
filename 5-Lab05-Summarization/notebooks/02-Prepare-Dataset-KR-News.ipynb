{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4872e4-c90b-434b-bfe5-f88292fba385",
   "metadata": {},
   "source": [
    "# 데이터셋 준비하기\n",
    "- https://www.kaggle.com/code/mitanshuchakrawarty/fine-tune-llm-for-text-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da7aa7-1a2d-4db1-b011-59217c32a83a",
   "metadata": {},
   "source": [
    "## 1. 환경 셋업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b5f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n",
    "\n",
    "!huggingface-cli login --token {HF_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdcebd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TRANSFORMERS_CACHE'] = \"/home/ec2-user/SageMaker/.cache\" \n",
    "os.environ['HF_DATASETS_CACHE'] = \"/home/ec2-user/SageMaker/.cache\" \n",
    "os.environ['HF_HOME'] = \"/home/ec2-user/SageMaker/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a396bc4f-0b0a-4ffb-8c59-18ed6d0a968d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ec2-user/SageMaker/.cs/conda/envs/llama3_puy310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4b722-f752-4687-8fbe-12855566892c",
   "metadata": {},
   "source": [
    "## 2. 데이터 셋 준비\n",
    "### 데이터 셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3710c3d-71bb-47b1-97eb-a72ef6dcfd53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 22194\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2466\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['date', 'category', 'press', 'title', 'document', 'link', 'summary'],\n",
       "        num_rows: 2740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_dataset_name = \"daekeun-ml/naver-news-summarization-ko\"\n",
    "\n",
    "# dataset = load_dataset(huggingface_dataset_name, \"3.0.0\")\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94062514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2022-07-03 17:14:37',\n",
       " 'category': 'economy',\n",
       " 'press': 'YTN ',\n",
       " 'title': '추경호 중기 수출지원 총력 무역금융 40조 확대',\n",
       " 'document': '앵커 정부가 올해 하반기 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 했습니다. 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했습니다. 류환홍 기자가 보도합니다. 기자 수출은 최고의 실적을 보였지만 수입액이 급증하면서 올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록했습니다. 정부가 수출확대에 총력을 기울이기로 한 것은 원자재 가격 상승 등 대외 리스크가 가중되는 상황에서 수출 증가세 지속이야말로 한국경제의 회복을 위한 열쇠라고 본 것입니다. 추경호 경제부총리 겸 기획재정부 장관 정부는 우리 경제의 성장엔진인 수출이 높은 증가세를 지속할 수 있도록 총력을 다하겠습니다. 우선 물류 부담 증가 원자재 가격 상승 등 가중되고 있는 대외 리스크에 대해 적극 대응하겠습니다. 특히 중소기업과 중견기업 수출 지원을 위해 무역금융 규모를 연초 목표보다 40조 원 늘린 301조 원까지 확대하고 물류비 부담을 줄이기 위한 대책도 마련했습니다. 이창양 산업통상자원부 장관 국제 해상운임이 안정될 때까지 월 4척 이상의 임시선박을 지속 투입하는 한편 중소기업 전용 선복 적재 용량 도 현재보다 주당 50TEU 늘려 공급하겠습니다. 하반기에 우리 기업들의 수출 기회를 늘리기 위해 2 500여 개 수출기업을 대상으로 해외 전시회 참가를 지원하는 등 마케팅 지원도 벌이기로 했습니다. 정부는 또 이달 중으로 반도체를 비롯한 첨단 산업 육성 전략을 마련해 수출 증가세를 뒷받침하고 에너지 소비를 줄이기 위한 효율화 방안을 마련해 무역수지 개선에 나서기로 했습니다. YTN 류환홍입니다.',\n",
       " 'link': 'https://n.news.naver.com/mnews/article/052/0001759333?sid=101',\n",
       " 'summary': '올해 상반기 우리나라 무역수지는 역대 최악인 103억 달러 적자를 기록한 가운데, 정부가 하반기에 우리 경제의 버팀목인 수출 확대를 위해 총력을 기울이기로 결정한 가운데, 특히 수출 중소기업의 물류난 해소를 위해 무역금융 규모를 40조 원 이상 확대하고 물류비 지원과 임시선박 투입 등을 추진하기로 했다.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96451467-4fc0-49af-bedf-040850c11fbb",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 변형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eda452",
   "metadata": {},
   "source": [
    "### Chat Message 형태 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1181ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def format_instruction(system_prompt: str, article: str, summary: str):\n",
    "    message = [\n",
    "            {\n",
    "                'content': system_prompt,\n",
    "                'role': 'system'\n",
    "            },\n",
    "            {\n",
    "                'content': f'Please summarize the goals for journalist in this text:\\n\\n{article}',\n",
    "                'role': 'user'\n",
    "            },\n",
    "            {\n",
    "                'content': f'{summary}',\n",
    "                'role': 'assistant'\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    return message # json.dumps(message, indent=2) # json.dumps(message, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "# system_prompt = \"You are an AI assistant specialized in news articles. Your role is to provide accurate summaries and insights. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\"\n",
    "# article = \"Within three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkert's portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species composition—which was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. \\\"We can't ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there,\\\" says Rinkert.\"\n",
    "# summary = \"Scientists are studying nests hoping to learn about transitional habitats that could help restore the shoreline of San Francisco Bay.\"\n",
    "\n",
    "# print(format_instruction(system_prompt, article, summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a4269",
   "metadata": {},
   "source": [
    "### Chat Message 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fede492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date', 'category', 'press', 'title', 'document', 'link', 'summary']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add system message to each conversation\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "columns_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b5a43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_instruction_dataset(data_point):\n",
    "    system_prompt = \"You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": format_instruction(system_prompt, data_point[\"document\"],data_point[\"summary\"])\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return (\n",
    "        data.shuffle(seed=42)\n",
    "        .map(generate_instruction_dataset).remove_columns(columns_to_remove)\n",
    "    )    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390bfdf",
   "metadata": {},
   "source": [
    "##### 전체 데이터 셋에서 일부 데티터 추출 (짧은 실습을 위해서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75ddc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num_debug_samples = 10\n",
    "test_num_debug_samples = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0edb7874",
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPLYING PREPROCESSING ON WHOLE DATASET\n",
    "\n",
    "dataset[\"train\"] = process_dataset(dataset[\"train\"].select(range(train_num_debug_samples)))\n",
    "dataset[\"test\"] = process_dataset(dataset[\"validation\"])\n",
    "dataset[\"validation\"] = process_dataset(dataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f033fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1000 rows from the training split\n",
    "# train_dataset = dataset['train'].shuffle(seed=42).select([i for i in range(train_num_debug_samples)])\n",
    "\n",
    "# # Select 100 rows from the test and validation splits\n",
    "# test_dataset = dataset['test'].shuffle(seed=42).select([i for i in range(test_num_debug_samples)])\n",
    "# validation_dataset = dataset['validation'].shuffle(seed=42).select([i for i in range(test_num_debug_samples)])\n",
    "\n",
    "train_dataset = dataset['train'].select([i for i in range(train_num_debug_samples)])\n",
    "\n",
    "# Select 100 rows from the test and validation splits\n",
    "test_dataset = dataset['test'].select([i for i in range(test_num_debug_samples)])\n",
    "validation_dataset = dataset['validation'].select([i for i in range(test_num_debug_samples)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6deeef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 10\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 10\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['messages'],\n",
       "     num_rows: 10\n",
       " }))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset,test_dataset,validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7197aa4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.',\n",
       "   'role': 'system'},\n",
       "  {'content': 'Please summarize the goals for journalist in this text:\\n\\n4일부터 석달간 증권사 신용융자담보비율 유지의무 면제 금융당국이 코스피지수가 장중 2300 아래까지 떨어지자 주식시장 변동성을 완화하는 조치를 시행하기로 했다. 금융위원회는 1일 주식시장 마감 후 김소영 부위원장 주재로 증권 유관기관과 금융시장합동점검회의를 열고 변동성 완화조치를 시행하기로 했다고 밝혔다. 이에 따라 다음 주식시장 개장일인 오는 4일부터 9월30일까지 3개월간 증권사의 신용융자담보비율 유지의무가 면제된다. 신용융자담보비율 유지의무란 증권사가 일명 ‘빚투’ 빚내서 투자 자금인 신용융자를 시행할 때 담보를 140% 이상 확보하고 내규에서 정한 담보비율을 유지할 것을 요구하는 규제다. 유지의무 면제는 증권사의 신용융자 담보주식에 대한 과도한 반대매매를 억제하기 위한 조치다. 금융당국은 코로나19 사태가 본격화한 2020년 3월에도 이 같은 조치를 6개월간 시행한 바 있다. 7일부터 10월6일까지는 상장기업의 하루 자기주식 매수 주문 수량 한도 제한도 완화된다. 신탁취득 주식도 발행주식총수의 1% 이내에서 신탁재산 총액 범위 내로 한시적으로 확대된다. 이와 함께 금융위는 금융감독원과 한국거래소 합동으로 공매도 특별점검을 실시해 공매도 현황과 시장교란 가능성 등을 살펴보기로 했다. 현재 공매도는 지난해 5월 이후 제한적으로 실시되고 있다. 금융위와 금감원은 매주 금요일 금융시장합동점검회의를 열어 증시 등 금융시장상황을 점검할 예정이다. 금융당국은 “컨틴전시플랜에 따라 필요한 시장 변동성 완화 조치를 검토·시행해 나갈 것”이라고 말했다.',\n",
       "   'role': 'user'},\n",
       "  {'content': '1일 1일 금융위원회는 증권 유관기관과 금융시장합동점검회의를 열고 코스피지수가 장중 2300 아래까지 떨어지자 주식시장 변동성을 완화하는 조치를 시행하기로 하였으며, 이에 따라 다음 주식시장 개장일인 오는 4일부터 9월30일까지 3개월간 증권사의 신용융자담보비율 유지의무가 면제된다.',\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dda7e7",
   "metadata": {},
   "source": [
    "## 4. 데이터 셋을 JSON 으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8af3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_json:  ../data/naver-news-summarization-ko/train_dataset.json\n",
      "validation_data_json:  ../data/naver-news-summarization-ko/validation_dataset.json\n",
      "test_data_json:  ../data/naver-news-summarization-ko/test_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_name = huggingface_dataset_name.split(\"/\")[1]\n",
    "data_folder = os.path.join(\"../data/\",dataset_name)\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "train_data_json = os.path.join(data_folder, \"train_dataset.json\")\n",
    "validation_data_json = os.path.join(data_folder, \"validation_dataset.json\")\n",
    "test_data_json = os.path.join(data_folder, \"test_dataset.json\")\n",
    "print(\"train_data_json: \", train_data_json)\n",
    "print(\"validation_data_json: \", validation_data_json)\n",
    "print(\"test_data_json: \", test_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79e88d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 319.49ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 880.42ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 892.98ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "29089"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save datasets to disk \n",
    "train_dataset.to_json(train_data_json, orient=\"records\", force_ascii=False)\n",
    "validation_dataset.to_json(validation_data_json, orient=\"records\", force_ascii=False)\n",
    "test_dataset.to_json(test_data_json, orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c521c6f",
   "metadata": {},
   "source": [
    "### 다음 노트북에서 사용하기 위해 변수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70fb035e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'data_folder' (str)\n",
      "Stored 'train_data_json' (str)\n",
      "Stored 'validation_data_json' (str)\n",
      "Stored 'test_data_json' (str)\n"
     ]
    }
   ],
   "source": [
    "%store data_folder\n",
    "%store train_data_json \n",
    "%store validation_data_json \n",
    "%store test_data_json \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5c4cc",
   "metadata": {},
   "source": [
    "## 5. Option: 데이터 셋을 ChatTemplate 형태로 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165e725",
   "metadata": {},
   "source": [
    "### Chat Template 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea4ba09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% for message in messages %}{% if message['role'] == 'system' %}{{ message['content'] }}{% elif message['role'] == 'user' %}{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}{% elif message['role'] == 'assistant' %}{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\\n\\nAssistant: ' }}{% endif %}\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LLAMA_3_CHAT_TEMPLATE = (\n",
    "    \"{% for message in messages %}\"\n",
    "        \"{% if message['role'] == 'system' %}\"\n",
    "            \"{{ message['content'] }}\"\n",
    "        \"{% elif message['role'] == 'user' %}\"\n",
    "            \"{{ '\\n\\nHuman: ' + message['content'] +  eos_token }}\"\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"{{ '\\n\\nAssistant: '  + message['content'] +  eos_token  }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "    \"{{ '\\n\\nAssistant: ' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "LLAMA_3_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bfcc19",
   "metadata": {},
   "source": [
    "### Chat Template 으로 변형하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54451f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer        \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE\n",
    "\n",
    "# template dataset\n",
    "def template_dataset(examples):\n",
    "    return{\"text\":  tokenizer.apply_chat_template(examples[\"messages\"], tokenize=False)}\n",
    "\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "test_dataset = test_dataset.map(template_dataset, remove_columns=[\"messages\"])\n",
    "validation_dataset = validation_dataset.map(template_dataset, remove_columns=[\"messages\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380fc688",
   "metadata": {},
   "source": [
    "### 변형된 Chat Message 형태 예시 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  6\n",
      "You are an AI assistant specialized in news articles.Your role is to provide accurate summaries and insights in Korean. Please analyze the given text and provide concise, informative summaries that highlight the key goals and findings.\n",
      "\n",
      "Human: Please summarize the goals for journalist in this text:\n",
      "\n",
      "SK바이오사이언스가 글로벌 사업의 고도화를 위해 조직 개편을 단행했다. SK바이오사이언스는 기존 해외사업개발실을 BD Business Development 1 3실로 확대 재편하고 글로벌 규제 및 허가 전담 조직인 Global RA Regulatory Affairs 실을 신설한다고 1일 밝혔다. SK바이오사이언스는 지난해 코로나19 COVID 19 백신 위탁 생산 등으로 주목받는 백신 기업으로 부상하며 글로벌 사업의 영역과 규모가 급속도로 성장 중이다. 이러한 성장 속도에 맞춰 기존 전담 조직인 해외사업개발실을 보다 세분화 및 전문화하고자 BD 1 3실로 확대 재편했다. BD 1 3실은 앞으로 기존에 영위 중인 백신 사업뿐만 아니라 세포·유전자치료제 CGT 등 신규 사업에 대한 △글로벌 네트워크들과의 공동 개발 △신규 C D MO 수주 △개발 제품 상업화 등 다양한 영역의 사업을 고도화하고 실행력을 높이는 업무를 담당한다. 또한 Global RA실을 신설해 미국 유럽 등 해외 선진국의 GMP Good Manufacturing Practice 를 확보하는 등 국제적인 수준의 관련 인증 및 허가 획득에도 더욱 박차를 가할 방침이다. CMC팀도 신설됐다. CMC는 화학 Chemistry 제조 Manufacturing 품질 Control 의 약자다. CMC팀은 완제 의약품을 만드는 공정 개발 process development 과 품질 관리 quality control 부문에서 핵심적인 역할을 수행한다. 연구부터 임상 허가 생산 품질에 이르는 GMP 관련 제반 업무를 관리한다. SK바이오사이언스는 이번 조직 개편이 글로벌 탑티어 바이오 기업으로의 성장을 더욱 앞당기고 초격차 경쟁력 확보의 계기가 될 것으로 기대하고 있다. SK바이오사이언스는 지난달 29일 국내 최초 코로나19 백신인 스카이코비원 SKYCovione 멀티주 의 품목허가를 획득했다. 이어 국가출하승인 및 WHO 등 해외 승인을 통해 국내외 백신 시장에 본격 진출할 예정이다.<|end_of_text|>\n",
      "\n",
      "Assistant: SK바이오사이언스가 글로벌 사업의 고도화를 위해 기존 해외사업개발실을 백신사업뿐만 아니라 다양한 영역의 사업을 고도화하고 실행력을 높이는 업무를 담당하는 1 3실로 확대 재편하고 글로벌 규제 및 허가 전담 조직을 신설한다고 1일 밝혔다.<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# print random sample\n",
    "import random\n",
    "\n",
    "for index in random.sample(range(len(train_dataset)), 1):\n",
    "    print(\"index: \", index)\n",
    "    # index = 5343\n",
    "    print(train_dataset[index][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60aaa4-b654-49b3-bb87-34ad6a9f375c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88bd97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('llama3_puy310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "6daafc7ae2313787fa97137de7504cfa7c5a594d29476828201b4f7d7fb5c4e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
